{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluence Module Docker Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Confluence on a LOCAL Machine\n",
    "\n",
    "## Requirements\n",
    "* docker installed somewhere where you have sudo priveledges to the point where \"docker --version\" completes successfully\n",
    "* a python environment\n",
    "\n",
    "## Optional\n",
    "* a dockerhub account (free, good for initial transition to HPC, sharing, versioning)\n",
    "\n",
    "\n",
    "## Run Confluence \n",
    "### Alter 1. and 2. for your local setup\n",
    "1. Git fork all of the repos you want to run, make sure you have sudo priveledges on a machine where \"docker --version\" works (locally)\n",
    "2. Prep an empty_mnt directory to store confluence run (requires gdown package in environment) and clone modules of interest\n",
    "3. Run the \"build_and_push_images\" function of this notebook locally to build images\n",
    "4. Run the \"generate_run_scripts\" section of this notebook to create python submission scripts for each module\n",
    "5. Run the generate_all_modules_bash Confluence Driver Script Generator section of this notebook to create a .sh submission script that runs each of the modules one by one (the one click run, Linux/Mac friendly)\n",
    "6. Run single module or entirety of confluence based on 5. or 6.\n",
    "7. Need to run again, or on different reaches? Run all steps except gdown in part 2 and part 3 to create a new mnt, edit the reaches_of_interest file in mnt/input/ \n",
    "\n",
    "\n",
    "## Run Confluence Parallelized\n",
    "* See bottom section; replace generate_slurm_scripts with generate_slurm_scripts_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS IGNORE\n",
    "\n",
    "def clone_repos(github_name, repo_dir, repo_names, name_map, branch='main'):\n",
    "    \"\"\"Clone repositories with specified branch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    github_name : str\n",
    "        GitHub username or organization name\n",
    "    repo_dir : str\n",
    "        Directory to clone repos into\n",
    "    repo_names : list\n",
    "        List of repository names to clone\n",
    "    branch : str or dict, optional\n",
    "        Branch name to clone. Can be:\n",
    "        - A string: same branch for all repos (default: 'main')\n",
    "        - A dict: mapping repo name to specific branch\n",
    "    \"\"\"\n",
    "    os.makedirs(repo_dir, exist_ok=True)\n",
    "    \n",
    "    for name in repo_names:\n",
    "        path = os.path.join(repo_dir, name)\n",
    "        repo_name = name_map.get(name, name)\n",
    "        url = f'https://github.com/{github_name}/{repo_name}.git'\n",
    "        \n",
    "        # Determine which branch to use\n",
    "        if isinstance(branch, dict):\n",
    "            branch_name = branch.get(name, 'main')\n",
    "        else:\n",
    "            branch_name = branch\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            print(f'[Remove] Deleting existing {name} to overwrite...')\n",
    "            try:\n",
    "                shutil.rmtree(path)  # rm -rf\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {path} : {e.strerror}\")\n",
    "        \n",
    "        print(f'[Clone] Cloning {name} from branch {branch_name}...')\n",
    "        sp.run(['git', 'clone', '--branch', branch_name, url, name], cwd=repo_dir)\n",
    "\n",
    "\n",
    "\n",
    "def build_and_push_images(repo_directory:str, modules_to_run:list, docker_username:str, push:bool = True, custom_tag_name:str = 'latest'):\n",
    "    for a_repo_name in modules_to_run:\n",
    "        repo_path = os.path.join(repo_directory, a_repo_name)\n",
    "        docker_path = f'{docker_username}/{a_repo_name}:{custom_tag_name}'\n",
    "        build_cmd = ['docker', 'build','--quiet', '-f', os.path.join(repo_path, \"Dockerfile\"), '-t', docker_path, repo_path]\n",
    "        try:\n",
    "            sp.run(build_cmd)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Docker build failed...\\n\"\n",
    "                f\"Build Command: {build_cmd}\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        if push:\n",
    "            try:\n",
    "                push_cmd = ['docker', 'push','--quiet', docker_path]\n",
    "                sp.run(push_cmd)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Docker push failed...\\n\"\n",
    "                    f\"Push Command: {push_cmd}\\n\"\n",
    "                    f\"Error: {e}\"\n",
    "                )\n",
    "\n",
    "#FUNCTIONS\n",
    "def generate_local_run_scripts(\n",
    "    run: str,\n",
    "    modules_to_run: list,\n",
    "    target_modules: list,\n",
    "    script_jobs: dict,\n",
    "    base_dir: str,\n",
    "    repo_directory: str,\n",
    "    rebuild_docker: bool,\n",
    "    docker_username: str,\n",
    "    push: bool,\n",
    "    custom_tag_name: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate Python scripts to run Docker containers locally for each module.\n",
    "    Handles dynamic JSON file detection similar to SLURM version.\n",
    "    \"\"\"\n",
    "    \n",
    "    def to_docker_path(path):\n",
    "        p = str(path).replace('\\\\', '/')\n",
    "        if len(p) >= 2 and p[1] == ':':\n",
    "            p = '/' + p[0].lower() + p[2:]\n",
    "        return p\n",
    "    \n",
    "    # Directory structure\n",
    "    mnt_dir_native = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "    mnt_dir = to_docker_path(mnt_dir_native)\n",
    "    input_dir = os.path.join(mnt_dir_native, 'input')\n",
    "    sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "    logs_dir = os.path.join(mnt_dir_native, 'logs')\n",
    "\n",
    "    os.makedirs(sh_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(mnt_dir_native, 'logs'), exist_ok=True)  # use native path\n",
    "\n",
    "    # JSON file paths (similar to HPC version)\n",
    "    json_files = {\n",
    "        'reaches_of_interest': os.path.join(input_dir, 'reaches_of_interest.json'),\n",
    "        'expanded': os.path.join(input_dir, 'expanded_reaches_of_interest.json'),\n",
    "        'reaches': os.path.join(input_dir, 'reaches.json'),\n",
    "        'basin': os.path.join(input_dir, 'basin.json'),\n",
    "        'metrosets': os.path.join(input_dir, 'metrosets.json'),\n",
    "    }\n",
    "    \n",
    "    # Build Docker images if requested\n",
    "    if rebuild_docker:\n",
    "        print(\"Building Docker images...\")\n",
    "        build_and_push_images(\n",
    "            repo_directory=repo_directory,\n",
    "            modules_to_run=target_modules,\n",
    "            docker_username=docker_username,\n",
    "            push=push,\n",
    "            custom_tag_name=custom_tag_name\n",
    "        )\n",
    "    \n",
    "    # Command dictionary\n",
    "    command_dict = {\n",
    "        'expanded_setfinder': f'docker run -v {mnt_dir}/input:/data {docker_username}/setfinder:{custom_tag_name} -r reaches_of_interest.json -c continent.json -e -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i {{index}}',\n",
    "        'expanded_combine_data': f'docker run -v {mnt_dir}/input:/data {docker_username}/combine_data:{custom_tag_name} -d /data -e -s 17',\n",
    "        'input': f'docker run -v {mnt_dir}/input:/mnt/data {docker_username}/input:{custom_tag_name} -v 17 -r /mnt/data/expanded_reaches_of_interest.json -c SWOT_L2_HR_RiverSP_D -i {{index}}',\n",
    "        'non_expanded_setfinder': f'docker run -v {mnt_dir}/input:/data {docker_username}/setfinder:{custom_tag_name} -c continent.json -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i {{index}}',\n",
    "        'non_expanded_combine_data': f'docker run -v {mnt_dir}/input:/data {docker_username}/combine_data:{custom_tag_name} -d /data -s 17',\n",
    "        'prediagnostics': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/diagnostics/prediagnostics:/mnt/data/output {docker_username}/prediagnostics:{custom_tag_name} -r reaches.json -i {{index}}',\n",
    "        # 'unconstrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data {docker_username}/priors:{custom_tag_name} -r unconstrained -p usgs riggs -g -s local -i {{index}}',\n",
    "        # 'constrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data {docker_username}/priors:{custom_tag_name} -r constrained -p usgs riggs -g -s local -i {{index}}',\n",
    "        'metroman': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/output {docker_username}/metroman:{custom_tag_name} -r metrosets.json -s local -v -i {{index}}',\n",
    "        'metroman_consolidation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/flpe {docker_username}/metroman_consolidation:{custom_tag_name} -i {{index}}',\n",
    "        'unconstrained_momma': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output {docker_username}/momma:{custom_tag_name} -r reaches.json -m 3 -i {{index}}',\n",
    "        'constrained_momma': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output {docker_username}/momma:{custom_tag_name} -r reaches.json -m 3 -c -i {{index}}',\n",
    "        'sad': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sad:/mnt/data/output {docker_username}/sad:{custom_tag_name} --reachfile reaches.json --index {{index}}',\n",
    "        'hivdi': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/hivdi:/mnt/data/flpe/hivdi {docker_username}/hivdi:{custom_tag_name} /mnt/data/input/reaches.json --input-dir /mnt/data/input -i ${{index}}',\n",
    "        'sic4dvar': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sic4dvar:/mnt/data/output -v {mnt_dir}/logs:/mnt/data/logs {docker_username}/sic4dvar:{custom_tag_name} -r reaches.json --index {{index}}',\n",
    "        'moi': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/output {docker_username}/moi:{custom_tag_name} -j basin.json -v -b unconstrained -i {{index}}',\n",
    "        'consensus': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe {docker_username}/consensus:{custom_tag_name} --mntdir /mnt/data -r /mnt/data/input/reaches.json -i {{index}}',\n",
    "        'unconstrained_offline': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/output {docker_username}/offline:{custom_tag_name} unconstrained timeseries integrator reaches.json {{index}}',\n",
    "        'validation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/output {docker_username}/validation:{custom_tag_name} reaches.json unconstrained {{index}}',\n",
    "        'output': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/diagnostics:/mnt/data/diagnostics -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/validation -v {mnt_dir}/output:/mnt/data/output {docker_username}/output:{custom_tag_name} -s local -j /app/metadata/metadata.json -m input momma metroman sic4dvar consensus swot -v 17 -i {{index}}'\n",
    "    }\n",
    "    \n",
    "    output_paths = []\n",
    "    \n",
    "    for module in modules_to_run:\n",
    "        if module not in command_dict:\n",
    "            print(f\"Warning: No command defined for module '{module}', skipping\")\n",
    "            continue\n",
    "        \n",
    "        job_count = script_jobs.get(module, \"1\")\n",
    "        \n",
    "        # Generate Python script with dynamic job count detection and logging support\n",
    "        script_content = f'''#!/usr/bin/env python3\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Module: {module}\n",
    "\n",
    "# Check for --log flag\n",
    "use_logging = '--log' in sys.argv\n",
    "logs_dir = r'{logs_dir}'\n",
    "\n",
    "# JSON file paths\n",
    "json_files = {{\n",
    "    'reaches_of_interest': r'{json_files['reaches_of_interest']}',\n",
    "    'expanded': r'{json_files['expanded']}',\n",
    "    'reaches': r'{json_files['reaches']}',\n",
    "    'basin': r'{json_files['basin']}',\n",
    "    'metrosets': r'{json_files['metrosets']}',\n",
    "}}\n",
    "\n",
    "def get_json_length(filepath):\n",
    "    \"\"\"Get length of JSON array file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return len(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {{filepath}}: {{e}}\")\n",
    "    return None\n",
    "\n",
    "# Determine job count for this module\n",
    "job_count = \"{job_count}\"\n",
    "\n",
    "if job_count == \"$default_jobs\":\n",
    "    # Dynamic job count based on module-specific logic\n",
    "    num_jobs = None\n",
    "    \n",
    "    # Module-specific JSON file selection (matching HPC logic)\n",
    "    if \"{module}\" == \"input\":\n",
    "        # Use expanded_reaches_of_interest.json if it exists\n",
    "        num_jobs = get_json_length(json_files['expanded'])\n",
    "        if num_jobs is None:\n",
    "            print(\"Error: expanded_reaches_of_interest.json not found for input module\")\n",
    "            print(\"Make sure expanded_combine_data has been run first\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    elif \"{module}\" == \"metroman\":\n",
    "        # Use metrosets.json if it exists, otherwise reaches.json, otherwise reaches_of_interest.json\n",
    "        num_jobs = get_json_length(json_files['metrosets'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "    \n",
    "    elif \"{module}\" == \"moi\":\n",
    "        # Use basin.json if it exists, otherwise reaches.json, otherwise reaches_of_interest.json\n",
    "        num_jobs = get_json_length(json_files['basin'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "    \n",
    "    else:\n",
    "        # For most modules: use reaches.json if exists, otherwise reaches_of_interest.json\n",
    "        num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "    \n",
    "    if num_jobs is None:\n",
    "        print(\"Error: Could not determine job count for module '{module}'\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"Determined {{num_jobs}} job(s) dynamically for module '{module}'\")\n",
    "else:\n",
    "    num_jobs = int(job_count)\n",
    "\n",
    "# Docker command template\n",
    "command_template = r\"\"\"{command_dict[module]}\"\"\"\n",
    "\n",
    "print(f\"\\\\nStarting module: {module}\")\n",
    "print(f\"Running {{num_jobs}} job(s)\")\n",
    "if use_logging:\n",
    "    print(f\"Logs will be written to: {{logs_dir}}\")\n",
    "print()\n",
    "\n",
    "for index in range(num_jobs):\n",
    "    print(f\"--- Running job {{index + 1}}/{{num_jobs}} for module '{module}' ---\")\n",
    "    \n",
    "    # Replace {{index}} with actual index\n",
    "    run_command = command_template.replace('{{index}}', str(index))\n",
    "    \n",
    "    if use_logging:\n",
    "        # Write output to log file\n",
    "        log_file = os.path.join(logs_dir, f\"{module}_{{index}}.log\")\n",
    "        print(f\"Logging to: {{log_file}}\")\n",
    "        \n",
    "        try:\n",
    "            with open(log_file, 'w') as f:\n",
    "                result = sp.run(run_command, shell=True, stdout=f, stderr=sp.STDOUT)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"Job {{index}} completed successfully\\\\n\")\n",
    "            else:\n",
    "                print(f\"Job {{index}} failed with exit code {{result.returncode}}\")\n",
    "                print(f\"Check log: {{log_file}}\\\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error running job {{index}}: {{e}}\\\\n\")\n",
    "    else:\n",
    "        # Direct output to terminal\n",
    "        print(f\"Command: {{run_command}}\")\n",
    "        \n",
    "        try:\n",
    "            result = sp.run(run_command, shell=True, check=True)\n",
    "            print(f\"Job {{index}} completed successfully\\\\n\")\n",
    "        except sp.CalledProcessError as e:\n",
    "            print(f\"Job {{index}} failed with exit code {{e.returncode}}\\\\n\")\n",
    "\n",
    "print(f\"All jobs completed for module '{module}'\")\n",
    "if use_logging:\n",
    "    print(f\"Logs saved in: {{logs_dir}}\")\n",
    "'''\n",
    "\n",
    "        output_script_path = os.path.join(sh_dir, f\"run_{module}.py\")\n",
    "        with open(output_script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        os.chmod(output_script_path, 0o755)\n",
    "        output_paths.append(output_script_path)\n",
    "        print(f\"Created: {output_script_path}\")\n",
    "    \n",
    "    return output_paths\n",
    "\n",
    "def generate_run_all_modules_script(\n",
    "    run: str,\n",
    "    modules_to_run: list,\n",
    "    script_jobs: dict,\n",
    "    base_dir: str,\n",
    "    script_name: str = \"run_all_modules.sh\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a bash script that runs all module scripts in series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    run : str\n",
    "        Run name\n",
    "    modules_to_run : list\n",
    "        List of modules\n",
    "    script_jobs : dict\n",
    "        Module -> job count mapping\n",
    "    base_dir : str\n",
    "        Base directory\n",
    "    script_name : str\n",
    "        Name of the generated script\n",
    "    \"\"\"\n",
    "    sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "    script_path = os.path.join(sh_dir, script_name)\n",
    "    \n",
    "    # Filter modules with non-zero job counts\n",
    "    filtered_modules = [m for m in modules_to_run if script_jobs.get(m, \"0\") != \"0\"]\n",
    "    \n",
    "    # Generate modules array\n",
    "    modules_array = \"modules_to_run=(\\n\"\n",
    "    for module in filtered_modules:\n",
    "        modules_array += f'    \"{module}\"\\n'\n",
    "    modules_array += \")\\n\"\n",
    "    \n",
    "    script_content = f\"\"\"#!/bin/bash\n",
    "# {script_name}\n",
    "# Runs all module scripts in series for run: {run}\n",
    "\n",
    "SCRIPT_DIR=\"{sh_dir}\"\n",
    "\n",
    "{modules_array}\n",
    "\n",
    "echo \"Starting Confluence Run: {run}\"\n",
    "\n",
    "for module in \"${{modules_to_run[@]}}\"; do\n",
    "    script_path=\"${{SCRIPT_DIR}}/run_${{module}}.py\"\n",
    "    \n",
    "    if [[ -f \"$script_path\" ]]; then\n",
    "        echo \"Running module: $module\"\n",
    "        echo \"Script: $script_path\"\n",
    "        python3 \"$script_path\" \"$@\"\n",
    "        \n",
    "        if [[ $? -ne 0 ]]; then\n",
    "            echo \"Error occurred while running $module. Exiting.\"\n",
    "        fi\n",
    "        \n",
    "        echo \"Finished module: $module\"\n",
    "        echo \"\"\n",
    "    else\n",
    "        echo \"Script not found for module: $module. Skipping.\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"All modules finished!\"\n",
    "\"\"\"\n",
    "    \n",
    "    with open(script_path, \"w\") as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    os.chmod(script_path, 0o755)\n",
    "    print(f\"Created: {script_path}\")\n",
    "    return script_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ALL DIRECTORES AND NAMES TO MATCH YOUR LOCAL SETUP\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess as sp\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import gdown\n",
    "import sys\n",
    "\n",
    "#Make initial folders\n",
    "BASE_DIR = Path('/Users/elisafriedmann/Documents/confluence') #Windows #Path(r'C:\\yourPath\\confluence') #Mac: #Path('/yourPath/confluence') #directory storing confluence runs\n",
    "REPO_DIR = BASE_DIR / 'modules'  #directory storing repos\n",
    "\n",
    "# Build directories\n",
    "for d in [BASE_DIR, REPO_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(BASE_DIR)\n",
    "\n",
    "RUN_NAME = 'runTest' #Specific run name i.e. 'test'\n",
    "run_dir = BASE_DIR / f'confluence_{RUN_NAME}' # new directory for run\n",
    "src_dir = BASE_DIR / 'confluence_empty'\n",
    "\n",
    "#------------------------------------------------\n",
    "\n",
    "# SETUP, GitHub, DOCKER (DOCKER MUST BE OPEN)\n",
    "github_name = 'efried130' # GitHub username or organization name where repos are located\n",
    "build = False # Only select True if want to store images on dockerhub (one way to move to HPC)\n",
    "docker_username = 'efrie130'\n",
    "custom_tag_name = 'vD' # version control, will default to 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose modules of interest to run\n",
    "\n",
    "#Name of confluence offline module\n",
    "#expanded and non_expanded modules each work from single 'setfinder' and 'combine_data' module\n",
    "INCLUDED_MODULES = [\n",
    "    'expanded_setfinder',\n",
    "    'expanded_combine_data',\n",
    "    'input',\n",
    "    'non_expanded_setfinder',\n",
    "    'non_expanded_combine_data',\n",
    "    'prediagnostics',\n",
    "    # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'unconstrained_momma',\n",
    "    # 'hivdi',\n",
    "    # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # 'moi',\n",
    "    # 'unconstrained_offline',\n",
    "    # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n",
    "# Git modules to pull\n",
    "TARGET_MODULES = [\n",
    "    'setfinder',\n",
    "    'combine_data',\n",
    "    'input',\n",
    "    'prediagnostics',\n",
    "    # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'momma',\n",
    "    # 'hivdi',\n",
    "    # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # 'moi',\n",
    "    # 'offline',\n",
    "    # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n",
    "# Pull working branches for certain Git repos\n",
    "branch_map = {\n",
    "    'setfinder': 'main',\n",
    "    'combine_data': 'main',\n",
    "    'input': 'input_D_products',\n",
    "    'prediagnostics': 'main',\n",
    "    # 'priors': 'main',\n",
    "    'metroman': 'main',\n",
    "    'metroman_consolidation': 'main',\n",
    "    'momma': 'main',\n",
    "    # 'hivdi': 'main',\n",
    "    # 'sad': 'main',\n",
    "    'sic4dvar': 'main',\n",
    "    'consensus': 'main',\n",
    "    # 'moi': 'main',\n",
    "    # 'offline': 'main',\n",
    "    # 'validation': 'main',\n",
    "    'output': 'add-sword-version'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download Confluence Directory and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=10gJwg0wsl51K_mcoXGq1uQVW34oQwrJc\n",
      "From (redirected): https://drive.google.com/uc?id=10gJwg0wsl51K_mcoXGq1uQVW34oQwrJc&confirm=t&uuid=e55abaad-18f4-4511-a4ec-fab11f134d33\n",
      "To: /Users/elisafriedmann/Documents/confluence/confluence_empty.tar.gz\n",
      "100%|██████████████████████████████████████| 2.47G/2.47G [01:34<00:00, 26.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "## INITIAL OR NEW MNT DOWNLOAD:\n",
    "\n",
    "## Only run once (large files)\n",
    "# Install empty /mnt directory with input data and eventual output data\n",
    "# Can also run in command line\n",
    "###############################\n",
    "\n",
    "# ! pip install gdown\n",
    "! gdown 10gJwg0wsl51K_mcoXGq1uQVW34oQwrJc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/pmsrh9js291bn65_6fkwxp200000gn/T/ipykernel_46306/2002017396.py:11: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=src_dir.parent)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/elisafriedmann/Documents/confluence/confluence_runTest/runTest_mnt')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "## SUBSEQUENT RUNS:\n",
    "# Copy the empty mount you just downloaded \n",
    "# Use this to preserve the empty mount as the base structure\n",
    "# Do this every time you need to run a new set of reaches\n",
    "####################\n",
    "\n",
    "## Extract from tar.gz\n",
    "tar_path = src_dir.with_suffix('.tar.gz')\n",
    "with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path=src_dir.parent)\n",
    "\n",
    "## Rename to your run\n",
    "src_dir.rename(run_dir)  # Rename to your run directory\n",
    "p = run_dir / \"empty_mnt\" # rename internal mnt to run name\n",
    "p.rename(p.with_name(f\"{RUN_NAME}_mnt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME: runTest\n",
      "REPO_DIR: /Users/elisafriedmann/Documents/confluence/modules\n",
      "SIF_DIR: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sif\n"
     ]
    }
   ],
   "source": [
    "# Point to necessary directories \n",
    "SIF_DIR = run_dir / 'sif' # Store built Docker images\n",
    "sh_dir = run_dir / 'sh_scripts' # Store the sh scripts to run each module\n",
    "report_dir = run_dir / 'report' # Job logs\n",
    "mnt_dir = run_dir / f'{RUN_NAME}_mnt' #the mnt storing all confluence run data\n",
    "\n",
    "os.environ['RUN_NAME'] = RUN_NAME\n",
    "os.environ['BASE_DIR'] = str(BASE_DIR)\n",
    "os.environ['REPO_DIR'] = str(REPO_DIR)\n",
    "os.environ['SIF_DIR'] = str(SIF_DIR)\n",
    "\n",
    "print(f'RUN_NAME: {RUN_NAME}')\n",
    "print(f'REPO_DIR: {REPO_DIR}')\n",
    "print(f'SIF_DIR: {SIF_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning setfinder from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'setfinder'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning combine_data from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'combine_data'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning input from branch input_D_products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'input'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning prediagnostics from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'prediagnostics'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning metroman from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'metroman'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning metroman_consolidation from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'metroman_consolidation'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning momma from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'momma'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning sic4dvar from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'sic4dvar'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning consensus from branch main...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'consensus'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clone] Cloning output from branch add-sword-version...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'output'...\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "## Clone repos (modules) from GitHub \n",
    "# only need to run ONCE unless you want to pull new branches or new modules\n",
    "########################\n",
    "\n",
    "name_map = {\n",
    "        'offline': 'offline-discharge-data-product-creation',\n",
    "        'moi': 'MOI',\n",
    "        'validation': 'Validation',\n",
    "        'hivdi': 'h2ivdi'\n",
    "    } # docker images must be lower case, should not change unless new names or new modules added\n",
    "\n",
    "clone_repos(github_name=github_name, repo_dir=REPO_DIR, repo_names=TARGET_MODULES, name_map=name_map, branch=branch_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build/Push modules to Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Generate Docker images from cloned modules \n",
    "# Only need to run ONCE unless you changed a module and are testing through docker\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "build_and_push_images(\\\n",
    "                      repo_directory = REPO_DIR, \\\n",
    "                      modules_to_run = TARGET_MODULES, \\\n",
    "                      docker_username = docker_username, \\\n",
    "                      push = build, \\\n",
    "                      custom_tag_name = custom_tag_name \\\n",
    "                     )\n",
    "                      \n",
    "# The output should look something like \n",
    "# sha256:6900c3d99325a4a7c8b282d4a7a62f2a0f3fc673f03f5ca3333c2746bf20d06a\n",
    "# docker.io/travissimmons/setfinder:latest\n",
    "\n",
    "\n",
    "#command line version of build_and_push_images function above - verbose, remove push if not using DockerHub, change tag name latest as needed\n",
    "# ! docker build -t efrie130/validation:latest ./Validation/ && docker push efrie130/validation:latest & "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_expanded_setfinder.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_expanded_combine_data.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_input.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_non_expanded_setfinder.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_non_expanded_combine_data.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_prediagnostics.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_metroman.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_metroman_consolidation.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_unconstrained_momma.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_sic4dvar.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_consensus.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_output.py\n",
      "Created: D:\\confluence\\confluence_runTest\\sh_scripts\\run_all_modules.sh\n",
      "\n",
      "All scripts generated!\n"
     ]
    }
   ],
   "source": [
    "# Define hardcoded job counts\n",
    "HARDCODED_JOBS = {\n",
    "    \"expanded_setfinder\": \"6\",\n",
    "    \"expanded_combine_data\": \"1\",\n",
    "    \"non_expanded_setfinder\": \"6\",\n",
    "    \"non_expanded_combine_data\": \"1\",\n",
    "    \"unconstrained_priors\": \"6\",\n",
    "    \"constrained_priors\": \"6\",\n",
    "    \"metroman_consolidation\": \"6\",\n",
    "    \"output\": \"6\",\n",
    "}\n",
    "\n",
    "# Define dynamic modules\n",
    "DYNAMIC_MODULES = [\n",
    "    \"input\",\n",
    "    \"prediagnostics\",\n",
    "    \"metroman\",\n",
    "    \"sic4dvar\",\n",
    "    \"unconstrained_momma\",\n",
    "    \"constrained_momma\",\n",
    "    \"sad\",\n",
    "    \"moi\",\n",
    "    \"consensus\",\n",
    "    \"unconstrained_offline\",\n",
    "    \"validation\",\n",
    "]\n",
    "\n",
    "# Build script_jobs dict\n",
    "script_jobs = {}\n",
    "for module in INCLUDED_MODULES:\n",
    "    if module in HARDCODED_JOBS:\n",
    "        script_jobs[module] = HARDCODED_JOBS[module]\n",
    "    elif module in DYNAMIC_MODULES:\n",
    "        script_jobs[module] = \"$default_jobs\"\n",
    "\n",
    "# Generate scripts\n",
    "# Can rebuild docker here in one step if testing changes\n",
    "generate_local_run_scripts(\n",
    "    run=RUN_NAME,\n",
    "    modules_to_run=INCLUDED_MODULES,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=BASE_DIR,\n",
    "    repo_directory=REPO_DIR,\n",
    "    rebuild_docker=False, # Set to True if you want to rebuild Docker images (e.g. if you made changes to the code)\n",
    "    docker_username=docker_username,\n",
    "    push=False,\n",
    "    custom_tag_name=custom_tag_name\n",
    ")\n",
    "\n",
    "# Generate master run script\n",
    "generate_run_all_modules_script(\n",
    "    run=RUN_NAME,\n",
    "    modules_to_run=INCLUDED_MODULES,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=BASE_DIR,\n",
    "    script_name=\"run_all_modules.sh\"\n",
    ")\n",
    "\n",
    "print(\"\\nAll scripts generated!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Confluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run modules - can use this cell or terminal\n",
    "# Note: scripts will usually run to completion, the logs directory will record actual errors from the run\n",
    "\n",
    "os.chdir(BASE_DIR)\n",
    "\n",
    "# Option 1 - Run ONE MODULE (i.e. first module)\n",
    "# !python \"{sh_dir}\\run_expanded_setfinder.py\" --log\n",
    "\n",
    "# Option 2 - Run multiple modules\n",
    "# Can be paralellized (but best results when modules run in serial)!\n",
    "\n",
    "#Linux/mac option\n",
    "# !\"{sh_dir}/run_all_modules.sh\" --log\n",
    "\n",
    "# Windows option\n",
    "import sys\n",
    "for mod in INCLUDED_MODULES:\n",
    "    result = sp.run(\n",
    "        [sys.executable, os.path.join(sh_dir, f'run_{mod}.py'), '--log']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Modules \n",
    "1. Create and run confluence through the minimum number of modules (i.e. input or prediag) as baseline\n",
    "2. Change module script and point inputs to 1. and outputs to new directory (symlink can be helpful!)\n",
    "3. Run analysis by modifying the python run scripts above or run full module by modifying docker command in command_dict both within generate_local_run_scripts function\n",
    "4. Change modules included in 'output' using the command_dict (generaete_local_run_scripts function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallellizing Modules\n",
    "\n",
    "* If you need to run many reaches, adding a parallel componenet to each .py script may be necessary\n",
    "* Change MAX_WORKERS argument in generate_local_run_scripts_parallel to fit your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hardcoded job counts (mostly per continent)\n",
    "HARDCODED_JOBS = {\n",
    "    \"expanded_setfinder\": \"6\",\n",
    "    \"expanded_combine_data\": \"1\",\n",
    "    \"non_expanded_setfinder\": \"6\",\n",
    "    \"non_expanded_combine_data\": \"1\",\n",
    "    \"unconstrained_priors\": \"6\",\n",
    "    \"constrained_priors\": \"6\",\n",
    "    \"metroman_consolidation\": \"6\",\n",
    "    \"output\": \"6\",\n",
    "}\n",
    "\n",
    "# Define dynamic modules (usually per reach/basin)\n",
    "DYNAMIC_MODULES = [\n",
    "    \"input\",\n",
    "    \"prediagnostics\",\n",
    "    \"metroman\",\n",
    "    \"sic4dvar\",\n",
    "    \"unconstrained_momma\",\n",
    "    \"constrained_momma\",\n",
    "    \"sad\",\n",
    "    \"moi\",\n",
    "    \"consensus\",\n",
    "    \"unconstrained_offline\",\n",
    "    \"validation\",\n",
    "]\n",
    "\n",
    "# Build script_jobs dict\n",
    "script_jobs = {}\n",
    "for module in INCLUDED_MODULES:\n",
    "    if module in HARDCODED_JOBS:\n",
    "        script_jobs[module] = HARDCODED_JOBS[module]\n",
    "    elif module in DYNAMIC_MODULES:\n",
    "        script_jobs[module] = \"$default_jobs\"\n",
    "        \n",
    "# #Sample function\n",
    "# # Change to match local needs\n",
    "def generate_local_run_scripts_parallel(\n",
    "    run: str,\n",
    "    modules_to_run: list,\n",
    "    target_modules: list,\n",
    "    script_jobs: dict,\n",
    "    base_dir: str,\n",
    "    repo_directory: str,\n",
    "    rebuild_docker: bool,\n",
    "    docker_username: str,\n",
    "    push: bool,\n",
    "    custom_tag_name: str,\n",
    "    max_workers: int = 4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate Python scripts to run Docker containers locally for each module.\n",
    "    Handles dynamic JSON file detection similar to SLURM version.\n",
    "    Uses multiprocessing for parallel job execution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_workers : int\n",
    "        Number of parallel workers for all modules (default: 4)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def to_docker_path(path):\n",
    "        p = str(path).replace('\\\\', '/')\n",
    "        if len(p) >= 2 and p[1] == ':':\n",
    "            p = '/' + p[0].lower() + p[2:]\n",
    "        return p\n",
    "    \n",
    "    # Directory structure\n",
    "    mnt_dir_native = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')\n",
    "    mnt_dir = to_docker_path(mnt_dir_native)\n",
    "    input_dir = os.path.join(mnt_dir_native, 'input')\n",
    "    sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "    logs_dir = os.path.join(mnt_dir_native, 'logs')\n",
    "\n",
    "    os.makedirs(sh_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(mnt_dir_native, 'logs'), exist_ok=True)  # use native path\n",
    "\n",
    "    # JSON file paths (similar to HPC version)\n",
    "    json_files = {\n",
    "        'reaches_of_interest': os.path.join(input_dir, 'reaches_of_interest.json'),\n",
    "        'expanded': os.path.join(input_dir, 'expanded_reaches_of_interest.json'),\n",
    "        'reaches': os.path.join(input_dir, 'reaches.json'),\n",
    "        'basin': os.path.join(input_dir, 'basin.json'),\n",
    "        'metrosets': os.path.join(input_dir, 'metrosets.json'),\n",
    "    }\n",
    "    \n",
    "    # Build Docker images if requested\n",
    "    if rebuild_docker:\n",
    "        print(\"Building Docker images...\")\n",
    "        build_and_push_images(\n",
    "            repo_directory=repo_directory,\n",
    "            modules_to_run=target_modules,\n",
    "            docker_username=docker_username,\n",
    "            push=push,\n",
    "            custom_tag_name=custom_tag_name\n",
    "        )\n",
    "    \n",
    "    # Command dictionary\n",
    "    command_dict = {\n",
    "        'expanded_setfinder': f'docker run -v {mnt_dir}/input:/data {docker_username}/setfinder:{custom_tag_name} -r reaches_of_interest.json -c continent.json -e -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i {{index}}',\n",
    "        'expanded_combine_data': f'docker run -v {mnt_dir}/input:/data {docker_username}/combine_data:{custom_tag_name} -d /data -e -s 17',\n",
    "        'input': f'docker run -v {mnt_dir}/input:/mnt/data {docker_username}/input:{custom_tag_name} -v 17 -r /mnt/data/expanded_reaches_of_interest.json -c SWOT_L2_HR_RiverSP_D -i {{index}}',\n",
    "        'non_expanded_setfinder': f'docker run -v {mnt_dir}/input:/data {docker_username}/setfinder:{custom_tag_name} -c continent.json -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i {{index}}',\n",
    "        'non_expanded_combine_data': f'docker run -v {mnt_dir}/input:/data {docker_username}/combine_data:{custom_tag_name} -d /data -s 17',\n",
    "        'prediagnostics': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/diagnostics/prediagnostics:/mnt/data/output {docker_username}/prediagnostics:{custom_tag_name} -r reaches.json -i {{index}}',\n",
    "        # 'unconstrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data {docker_username}/priors:{custom_tag_name} -r unconstrained -p usgs riggs -g -s local -i {{index}}',\n",
    "        # 'constrained_priors': f'docker run -v {mnt_dir}/input:/mnt/data {docker_username}/priors:{custom_tag_name} -r constrained -p usgs riggs -g -s local -i {{index}}',\n",
    "        'metroman': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/output {docker_username}/metroman:{custom_tag_name} -r metrosets.json -s local -v -i {{index}}',\n",
    "        'metroman_consolidation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/metroman:/mnt/data/flpe {docker_username}/metroman_consolidation:{custom_tag_name} -i {{index}}',\n",
    "        'unconstrained_momma': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output {docker_username}/momma:{custom_tag_name} -r reaches.json -m 3 -i {{index}}',\n",
    "        'constrained_momma': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/momma:/mnt/data/output {docker_username}/momma:{custom_tag_name} -r reaches.json -m 3 -c -i {{index}}',\n",
    "        'sad': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sad:/mnt/data/output {docker_username}/sad:{custom_tag_name} --reachfile reaches.json --index {{index}}',\n",
    "        'hivdi': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/hivdi:/mnt/data/flpe/hivdi {docker_username}/hivdi:{custom_tag_name} /mnt/data/input/reaches.json --input-dir /mnt/data/input -i ${{index}}',\n",
    "        'sic4dvar': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe/sic4dvar:/mnt/data/output -v {mnt_dir}/logs:/mnt/data/logs {docker_username}/sic4dvar:{custom_tag_name} -r reaches.json --index {{index}}',\n",
    "        'moi': f'docker run --env AWS_BATCH_JOB_ID=\"foo\" -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/output {docker_username}/moi:{custom_tag_name} -j basin.json -v -b unconstrained -i {{index}}',\n",
    "        'consensus': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe {docker_username}/consensus:{custom_tag_name} --mntdir /mnt/data -r /mnt/data/input/reaches.json -i {{index}}',\n",
    "        'unconstrained_offline': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/output {docker_username}/offline:{custom_tag_name} unconstrained timeseries integrator reaches.json {{index}}',\n",
    "        'validation': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/output {docker_username}/validation:{custom_tag_name} reaches.json unconstrained {{index}}',\n",
    "        'output': f'docker run -v {mnt_dir}/input:/mnt/data/input -v {mnt_dir}/flpe:/mnt/data/flpe -v {mnt_dir}/moi:/mnt/data/moi -v {mnt_dir}/diagnostics:/mnt/data/diagnostics -v {mnt_dir}/offline:/mnt/data/offline -v {mnt_dir}/validation:/mnt/data/validation -v {mnt_dir}/output:/mnt/data/output {docker_username}/output:{custom_tag_name} -s local -j /app/metadata/metadata.json -m input momma metroman sic4dvar consensus swot -v 17 -i {{index}}'\n",
    "    }\n",
    "\n",
    "    output_paths = []\n",
    "\n",
    "    for module in modules_to_run:\n",
    "        if module not in command_dict:\n",
    "            print(f\"Warning: No command defined for module '{module}', skipping\")\n",
    "            continue\n",
    "\n",
    "        job_count = script_jobs.get(module, \"1\")\n",
    "\n",
    "        script_content = f'''#!/usr/bin/env python3\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Module: {module}\n",
    "\n",
    "# Check for --log flag\n",
    "use_logging = '--log' in sys.argv\n",
    "logs_dir = r'{logs_dir}'\n",
    "\n",
    "# JSON file paths\n",
    "json_files = {{\n",
    "    'reaches_of_interest': r'{json_files['reaches_of_interest']}',\n",
    "    'expanded': r'{json_files['expanded']}',\n",
    "    'reaches': r'{json_files['reaches']}',\n",
    "    'basin': r'{json_files['basin']}',\n",
    "    'metrosets': r'{json_files['metrosets']}',\n",
    "}}\n",
    "\n",
    "def get_json_length(filepath):\n",
    "    \"\"\"Get length of JSON array file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return len(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {{filepath}}: {{e}}\")\n",
    "    return None\n",
    "\n",
    "# Determine job count for this module\n",
    "job_count = \"{job_count}\"\n",
    "\n",
    "if job_count == \"$default_jobs\":\n",
    "    num_jobs = None\n",
    "\n",
    "    if \"{module}\" == \"input\":\n",
    "        num_jobs = get_json_length(json_files['expanded'])\n",
    "        if num_jobs is None:\n",
    "            print(\"Error: expanded_reaches_of_interest.json not found for input module\")\n",
    "            print(\"Make sure expanded_combine_data has been run first\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    elif \"{module}\" == \"metroman\":\n",
    "        num_jobs = get_json_length(json_files['metrosets'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "\n",
    "    elif \"{module}\" == \"moi\":\n",
    "        num_jobs = get_json_length(json_files['basin'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "\n",
    "    else:\n",
    "        num_jobs = get_json_length(json_files['reaches'])\n",
    "        if num_jobs is None:\n",
    "            num_jobs = get_json_length(json_files['reaches_of_interest'])\n",
    "\n",
    "    if num_jobs is None:\n",
    "        print(\"Error: Could not determine job count for module '{module}'\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"Determined {{num_jobs}} job(s) dynamically for module '{module}'\")\n",
    "else:\n",
    "    num_jobs = int(job_count)\n",
    "\n",
    "# Docker command template\n",
    "command_template = r\"\"\"{command_dict[module]}\"\"\"\n",
    "\n",
    "MAX_WORKERS = {max_workers}\n",
    "\n",
    "def run_job(index):\n",
    "    \"\"\"Run a single Docker job. Designed to be called by multiprocessing.Pool.\"\"\"\n",
    "    run_command = command_template.replace('{{index}}', str(index))\n",
    "\n",
    "    if use_logging:\n",
    "        log_file = os.path.join(logs_dir, f\"{module}_{{index}}.log\")\n",
    "        try:\n",
    "            with open(log_file, 'w') as f:\n",
    "                result = sp.run(run_command, shell=True, stdout=f, stderr=sp.STDOUT)\n",
    "            return index, result.returncode, log_file\n",
    "        except Exception as e:\n",
    "            return index, -1, str(e)\n",
    "    else:\n",
    "        try:\n",
    "            result = sp.run(run_command, shell=True, capture_output=True, text=True)\n",
    "            if result.stdout:\n",
    "                print(result.stdout, flush=True)\n",
    "            if result.stderr:\n",
    "                print(result.stderr, flush=True)\n",
    "            return index, result.returncode, None\n",
    "        except Exception as e:\n",
    "            return index, -1, str(e)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\\\nStarting module: {module}\")\n",
    "    print(f\"Running {{num_jobs}} job(s) with {{MAX_WORKERS}} parallel workers\")\n",
    "    if use_logging:\n",
    "        print(f\"Logs will be written to: {{logs_dir}}\")\n",
    "    print()\n",
    "\n",
    "    indices = list(range(num_jobs))\n",
    "    failed_jobs = []\n",
    "\n",
    "    with Pool(processes=MAX_WORKERS) as pool:\n",
    "        for index, returncode, info in pool.imap_unordered(run_job, indices):\n",
    "            if returncode == 0:\n",
    "                print(f\"[OK] Job {{index}} completed successfully\", flush=True)\n",
    "            else:\n",
    "                failed_jobs.append(index)\n",
    "                if use_logging:\n",
    "                    print(f\"[FAIL] Job {{index}} failed (exit {{returncode}}) — log: {{info}}\", flush=True)\n",
    "                else:\n",
    "                    print(f\"[FAIL] Job {{index}} failed (exit {{returncode}}): {{info}}\", flush=True)\n",
    "\n",
    "    print(f\"\\\\nAll jobs finished for module '{module}'\")\n",
    "    print(f\"  Succeeded: {{num_jobs - len(failed_jobs)}}/{{num_jobs}}\")\n",
    "    if failed_jobs:\n",
    "        print(f\"  Failed jobs: {{failed_jobs}}\")\n",
    "    if use_logging:\n",
    "        print(f\"  Logs saved in: {{logs_dir}}\")\n",
    "\n",
    "\n",
    "# Required for multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "        output_script_path = os.path.join(sh_dir, f\"run_{module}.py\")\n",
    "        with open(output_script_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "\n",
    "        os.chmod(output_script_path, 0o755)\n",
    "        output_paths.append(output_script_path)\n",
    "        print(f\"Created: {output_script_path}\")\n",
    "\n",
    "    return output_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_expanded_setfinder.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_expanded_combine_data.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_input.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_non_expanded_setfinder.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_non_expanded_combine_data.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_prediagnostics.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_metroman.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_metroman_consolidation.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_unconstrained_momma.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_sic4dvar.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_consensus.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_output.py\n",
      "Created: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_all_modules.sh\n",
      "\n",
      "All scripts generated!\n",
      "Starting Confluence Run: runTest\n",
      "Running module: expanded_setfinder\n",
      "Script: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_expanded_setfinder.py\n",
      "\n",
      "Starting module: expanded_setfinder\n",
      "Running 6 job(s) with 4 parallel workers\n",
      "Logs will be written to: /Users/elisafriedmann/Documents/confluence/confluence_runTest/runTest_mnt/logs\n",
      "\n",
      "[OK] Job 1 completed successfully\n",
      "[OK] Job 0 completed successfully\n",
      "[OK] Job 2 completed successfully\n",
      "[OK] Job 3 completed successfully\n",
      "[OK] Job 4 completed successfully\n",
      "[OK] Job 5 completed successfully\n",
      "\n",
      "All jobs finished for module 'expanded_setfinder'\n",
      "  Succeeded: 6/6\n",
      "  Logs saved in: /Users/elisafriedmann/Documents/confluence/confluence_runTest/runTest_mnt/logs\n",
      "Finished module: expanded_setfinder\n",
      "\n",
      "Running module: expanded_combine_data\n",
      "Script: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_expanded_combine_data.py\n",
      "\n",
      "Starting module: expanded_combine_data\n",
      "Running 1 job(s) with 4 parallel workers\n",
      "Logs will be written to: /Users/elisafriedmann/Documents/confluence/confluence_runTest/runTest_mnt/logs\n",
      "\n",
      "[OK] Job 0 completed successfully\n",
      "\n",
      "All jobs finished for module 'expanded_combine_data'\n",
      "  Succeeded: 1/1\n",
      "  Logs saved in: /Users/elisafriedmann/Documents/confluence/confluence_runTest/runTest_mnt/logs\n",
      "Finished module: expanded_combine_data\n",
      "\n",
      "Running module: input\n",
      "Script: /Users/elisafriedmann/Documents/confluence/confluence_runTest/sh_scripts/run_input.py\n",
      "Determined 5 job(s) dynamically for module 'input'\n",
      "\n",
      "Starting module: input\n",
      "Running 5 job(s) with 4 parallel workers\n",
      "Logs will be written to: /Users/elisafriedmann/Documents/confluence/confluence_runTest/runTest_mnt/logs\n",
      "\n",
      "Determined 5 job(s) dynamically for module 'input'\n",
      "Determined 5 job(s) dynamically for module 'input'\n",
      "Determined 5 job(s) dynamically for module 'input'\n",
      "Determined 5 job(s) dynamically for module 'input'\n"
     ]
    }
   ],
   "source": [
    "generate_local_run_scripts_parallel(\n",
    "    run=RUN_NAME,\n",
    "    modules_to_run=INCLUDED_MODULES,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=BASE_DIR,\n",
    "    repo_directory=REPO_DIR,\n",
    "    rebuild_docker=False,\n",
    "    docker_username=docker_username,\n",
    "    push=False,\n",
    "    custom_tag_name=custom_tag_name,\n",
    "    max_workers=4, # Set this based on machine\n",
    ")\n",
    "\n",
    "# Generate master run script\n",
    "generate_run_all_modules_script(\n",
    "    run=RUN_NAME,\n",
    "    modules_to_run=INCLUDED_MODULES,\n",
    "    script_jobs=script_jobs,\n",
    "    base_dir=BASE_DIR,\n",
    "    script_name=\"run_all_modules.sh\"\n",
    ")\n",
    "\n",
    "print(\"\\nAll scripts generated!\")\n",
    "\n",
    "\n",
    "\n",
    "#Linux/mac option\n",
    "!\"{sh_dir}/run_all_modules.sh\" --log\n",
    "\n",
    "# Windows option\n",
    "# Log files will be saved in the logs directory, but will likely not log to cell\n",
    "\n",
    "# for mod in INCLUDED_MODULES:\n",
    "#     result = sp.run(\n",
    "#         [sys.executable, os.path.join(sh_dir, f'run_{mod}.py'), '--log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confluence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
