{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Confluence on an HPC\n",
    "\n",
    "# Requirements\n",
    "* docker and dockerHub account installed somewhere where you have sudo priveledges to the point where \"docker --version\" completes successfully OR GitHub\n",
    "* singularity or apptainer installed on your HPC\n",
    "* Basic python environment\n",
    "\n",
    "\n",
    "# Overall Tasks\n",
    "### Alter 1. and 2. for your local setup\n",
    "1. Git fork all of the repos you want to run, make sure you have sudo priveledges on a machine where \"docker --version\" works (locally)\n",
    "2. Prep an empty_mnt directory to store confluence run (requires gdown package in environment) and clone modules of interest\n",
    "3. Run the \"Prepare Images Locally\" section of the local notebook on GitHub\n",
    "4. Run the \"Confluence Module SLURM Script Generator\" section of this notebook on your HPC to create SLURM submission scripts for each module\n",
    "5. Run the Confluence Driver Script Generator section of this notebook on your HPC to create a SLURM submission script that runs each of the modules one by one (the one click run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Functions (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS IGNORE\n",
    "def build_and_push_images(repo_directory:str, target_repo_names:list, target_docker_names:list, docker_username:str, push:bool = True, custom_tag_name:str = 'latest'):\n",
    "    # Validate that lists are the same length\n",
    "    if len(target_repo_names) != len(target_docker_names):\n",
    "        raise ValueError(\"target_repo_names and target_docker_names must have the same length\")\n",
    "    \n",
    "    for a_repo_name, a_docker_name in zip(target_repo_names, target_docker_names):\n",
    "        repo_path = os.path.join(repo_directory, a_repo_name)\n",
    "        a_docker_name_lower = a_docker_name.lower()\n",
    "        docker_path = f'{docker_username}/{a_docker_name_lower}:{custom_tag_name}'\n",
    "        build_cmd = ['docker', 'build','--quiet', '-f', os.path.join(repo_path, \"Dockerfile\"), '-t', docker_path, repo_path]\n",
    "        try:\n",
    "            sp.run(build_cmd)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Docker build failed...\\n\"\n",
    "                f\"Build Command: {build_cmd}\\n\"\n",
    "                f\"Error: {e}\"\n",
    "            )\n",
    "        if push:\n",
    "            try:\n",
    "                push_cmd = ['docker', 'push','--quiet', docker_path]\n",
    "                sp.run(push_cmd)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Docker push failed...\\n\"\n",
    "                    f\"Push Command: {push_cmd}\\n\"\n",
    "                    f\"Error: {e}\"\n",
    "                )\n",
    "            \n",
    "def build_sifs_and_create_slurm_scripts(run_list, included_modules, base_dir, docker_username, build, custom_tag_name):\n",
    "\n",
    "    for run in run_list:\n",
    "        \n",
    "        # Fail safe directory creation\n",
    "        # Has to exist with 'mnt' structure (Do it exister avec la structure 'mnt')\n",
    "        mnt_dir = os.path.join(base_dir, f'confluence_{run}', f'{run}_mnt')       \n",
    "        # Create the sh_scripts directory (Cree le repertoire sh_scripts)\n",
    "        sh_dir = os.path.join(base_dir, f'confluence_{run}', 'sh_scripts')\n",
    "        if not os.path.exists(sh_dir):\n",
    "            os.makedirs(sh_dir)\n",
    "        # Create the sif directory (Cree la repertoire sif)\n",
    "        sif_dir = os.path.join(base_dir, f'confluence_{run}', 'sif')\n",
    "        if not os.path.exists(sif_dir):\n",
    "            os.makedirs(sif_dir)\n",
    "        # Create the report directory (Cree la repertoire report)\n",
    "        report_dir = os.path.join(base_dir, f'confluence_{run}', 'report')\n",
    "        if not os.path.exists(report_dir):\n",
    "            os.makedirs(report_dir)\n",
    "\n",
    "        # Create batchs script details\n",
    "        submission_prefix = '#SBATCH'\n",
    "\n",
    "        job_details = {\n",
    "        'partition': 'cpu-preempt',\n",
    "        'nodes' : '1',\n",
    "        'cpus-per-task': '1',\n",
    "        'job-name': f'{run}_cfl',\n",
    "        }\n",
    "        \n",
    "        command_dict = {\n",
    "            'expanded_setfinder': f'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -r reaches_of_interest.json -c continent.json -e -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'expanded_combine_data': f'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data  -e -s 17',\n",
    "            'input': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind ' + f'{mnt_dir}/input:/mnt/data ' + os.path.join(sif_dir, 'input.simg') + ' -v 17 -r /mnt/data/expanded_reaches_of_interest.json -c SWOT_L2_HR_RiverSP_D -i ${GLOBAL_INDEX}',\n",
    "            'non_expanded_setfinder': f'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.simg') + ' -c continent.json -s 17 -o /data -n /data -a MetroMan HiVDI SIC -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'non_expanded_combine_data': f'singularity run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.simg') + ' -d /data -s 17',\n",
    "            'prediagnostics': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, f'prediagnostics.simg') + ' -i ${GLOBAL_INDEX} -r reaches.json',\n",
    "            'constrained_priors': f'singularity run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.simg\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r constrained -p usgs riggs -g -s local',\n",
    "            'unconstrained_priors': f'singularity run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.simg\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r unconstrained -p usgs riggs -g -s local',\n",
    "            'hivdi': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/hivdi:/mnt/data/flpe/hivdi ' + os.path.join(sif_dir, 'hivdi.simg') + ' /mnt/data/input/reaches.json --input-dir /mnt/data/input -i ${SLURM_ARRAY_TASK_ID}',\n",
    "            'sic4dvar': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sic4dvar:/mnt/data/output,{mnt_dir}/logs:/mnt/data/logs '+ os.path.join(sif_dir, 'sic4dvar.simg') + ' -r reaches.json --index ${GLOBAL_INDEX}',\n",
    "            'metroman': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/output ' + os.path.join(sif_dir, \"metroman.simg\") + ' -i ${GLOBAL_INDEX} -r metrosets.json -s local -v',\n",
    "            'metroman_consolidation': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/flpe ' + os.path.join(sif_dir, 'metroman_consolidation.simg') + ' -i ${GLOBAL_INDEX}',\n",
    "            'unconstrained_momma': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/momma:/mnt/data/output ' + os.path.join(sif_dir, 'momma.simg') + ' -r reaches.json -m 3 -i ${GLOBAL_INDEX}',\n",
    "            'constrained_momma': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/momma:/mnt/data/output ' + os.path.join(sif_dir, 'momma.simg') + ' -r reaches.json -m 3 -c -i ${GLOBAL_INDEX}',\n",
    "            'sad': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sad:/mnt/data/output ' + os.path.join(sif_dir, 'sad.simg') + ' --reachfile reaches.json --index ${GLOBAL_INDEX}',\n",
    "            'moi': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/output ' + os.path.join(sif_dir, 'moi.simg') + ' -j basin.json -v -b unconstrained -i ${GLOBAL_INDEX}', # -s local\n",
    "            'consensus': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe ' + os.path.join(sif_dir, 'consensus.simg') + ' --mntdir /mnt/data -r /mnt/data/input/reaches.json -i ${GLOBAL_INDEX}',\n",
    "            'unconstrained_offline': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/output ' + os.path.join(sif_dir, 'offline.simg') + ' unconstrained timeseries integrator reaches.json ${GLOBAL_INDEX}',\n",
    "            'validation': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\nsingularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/output ' + os.path.join(sif_dir, 'validation.simg') + ' -r reaches.json -t unconstrained -i ${GLOBAL_INDEX}',\n",
    "            # 'output': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/diagnostics:/mnt/data/diagnostics,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/validation,{mnt_dir}/output:/mnt/data/output ' + os.path.join(sif_dir, 'output.simg') + ' -s local -j /app/metadata/metadata.json -m input prediagnostics momma hivdi neobam metroman sic4dvar sad consensus validation swot priors -v 17 -i ${SLURM_ARRAY_TASK_ID}'\n",
    "            'output': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/diagnostics:/mnt/data/diagnostics,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/validation,{mnt_dir}/output:/mnt/data/output ' + os.path.join(sif_dir, 'output.simg') + ' -s local -j /app/metadata/metadata.json -m input prediagnostics momma metroman sic4dvar consensus swot -v 17 -i ${SLURM_ARRAY_TASK_ID}'\n",
    "        }\n",
    "        \n",
    "        built_images = set() #prevents setfinder and combine_data redundant double build\n",
    "\n",
    "\n",
    "        def create_slurm_script(job_details=job_details, build_image=False, sif_dir='foo'):\n",
    "            submission_prefix = job_details['submission_prefix']\n",
    "            if build_image:\n",
    "                module_name = job_details['module_name']\n",
    "                image_name = module_name.replace('expanded_', '').replace('non_', '').replace('unconstrained_', '').replace('constrained_', '')\n",
    "                sp.run(['singularity', 'build', '-F', os.path.join(sif_dir, image_name + '.simg'), f\"docker://{job_details['docker_username']}/{image_name}:{custom_tag_name}\"])\n",
    "\n",
    "            file = open(os.path.join(sh_dir, f'{module_to_run}.sh'), 'w')\n",
    "            file.write('#!/bin/bash \\n')\n",
    "            file.write(f'{submission_prefix} -o {os.path.join(report_dir, f\"{module_to_run}.%j_%a.out\")}' + ' \\n')\n",
    "\n",
    "            for item in job_details:\n",
    "                if item not in ['run_command', 'module_name', 'docker_username', 'submission_prefix']:\n",
    "                    file.write(f'{submission_prefix} --{item}={job_details[item]} \\n')\n",
    "            file.write(job_details[\"run_command\"])\n",
    "            file.close()\n",
    "\n",
    "\n",
    "        for module_to_run, run_command in command_dict.items():\n",
    "            \n",
    "            if module_to_run == 'moi':\n",
    "                time_to_use = '00:30:00'\n",
    "                mem_to_use = '2G'\n",
    "            elif module_to_run == 'output':\n",
    "                time_to_use = '05:00:00'\n",
    "                mem_to_use = '4G'\n",
    "            else:\n",
    "                time_to_use = '00:20:00'\n",
    "                mem_to_use = '4G'\n",
    "                \n",
    "            if included_modules:\n",
    "                if module_to_run not in included_modules:\n",
    "                    continue\n",
    "\n",
    "            print('DIRECTORY NAME: ', run, '\\nMODULE: ', module_to_run)\n",
    "            \n",
    "\n",
    "\n",
    "            job_details.update({\n",
    "                'run_command': run_command,\n",
    "                'module_name': module_to_run,\n",
    "                'mem': mem_to_use,\n",
    "                'time': time_to_use,\n",
    "                'docker_username': docker_username,\n",
    "                'submission_prefix': submission_prefix,\n",
    "                'job-name': f'{module_to_run}_{run}_cfl',\n",
    "\n",
    "            })\n",
    "            \n",
    "            create_slurm_script(job_details=job_details, build_image=build, sif_dir=sif_dir)\n",
    "\n",
    "                \n",
    "def generate_slurm_driver(\n",
    "    job_name: str,\n",
    "    output_log_dir: str,\n",
    "    partition: str,\n",
    "    time_limit: str,\n",
    "    nodes: int,\n",
    "    ntasks: int,\n",
    "    cpus_per_task: int,\n",
    "    mem: str,\n",
    "    run: str,\n",
    "    directory: str,\n",
    "    json_file: str,\n",
    "    expanded_json_file: str,\n",
    "    reach_json_file: str,\n",
    "    basin_json_file: str,\n",
    "    metroman_json_file: str,\n",
    "    batch_size: int,\n",
    "    concurrent_jobs: int,\n",
    "    script_jobs: dict[str, str],\n",
    "    scripts: list[str]\n",
    ") -> str:\n",
    "    slurm_header = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --output={output_log_dir}/{job_name}_%j_%a.out\n",
    "#SBATCH --error={output_log_dir}/{job_name}_%j_%a.err\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --nodes={nodes}\n",
    "#SBATCH --ntasks={ntasks}\n",
    "#SBATCH --cpus-per-task={cpus_per_task}\n",
    "#SBATCH --mem={mem}\n",
    "\n",
    "run='{run}'\n",
    "echo \"Run: $run\"\n",
    "\n",
    "directory=\"{directory}\"\n",
    "\n",
    "# Parameters\n",
    "json_file=\"{json_file}\"\n",
    "expanded_json_file=\"{expanded_json_file}\"\n",
    "reach_json_file=\"{reach_json_file}\"\n",
    "basin_json_file=\"{basin_json_file}\"\n",
    "metroman_json_file=\"{metroman_json_file}\"\n",
    "default_jobs=$(jq length \"$json_file\")\n",
    "\n",
    "# Adjust to HPC requirements\n",
    "batch_size={batch_size}\n",
    "concurrent_jobs={concurrent_jobs}\n",
    "\n",
    "# Map specific script names to their job counts\n",
    "declare -A script_jobs=(\n",
    "\"\"\"\n",
    "\n",
    "    # Inject job counts into script_jobs associative array\n",
    "    for script, jobs in script_jobs.items():\n",
    "        slurm_header += f\"    [{script}]={jobs}\\n\"\n",
    "    slurm_header += \")\\n\\n\"\n",
    "\n",
    "    # Build scripts array\n",
    "    script_array = '    ' + '\\n    '.join(scripts)\n",
    "    scripts_block = f\"\"\"scripts=(\n",
    "{script_array}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "    body = rf\"\"\"{scripts_block}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for slurm_script in \"${{scripts[@]}}\"; do\n",
    "    echo \"Starting submission for: $slurm_script\"\n",
    "    date\n",
    "\n",
    "    # Initialize num_jobs from script_jobs array FIRST\n",
    "    num_jobs=\"${{script_jobs[$slurm_script]}}\"\n",
    "\n",
    "    # Dynamic job count updates (files created during workflow)\n",
    "    if [[ -s \"$expanded_json_file\" ]]; then\n",
    "      expanded_jobs=$(jq length \"$expanded_json_file\")\n",
    "      script_jobs[\"input.sh\"]=$expanded_jobs\n",
    "      # Update num_jobs if this is the input script\n",
    "      if [[ \"$slurm_script\" == \"input.sh\" ]]; then\n",
    "        num_jobs=$expanded_jobs\n",
    "      fi\n",
    "    fi\n",
    "\n",
    "    if [[ -s \"$basin_json_file\" ]]; then\n",
    "      basin_jobs=$(jq length \"$basin_json_file\")\n",
    "      script_jobs[\"moi.sh\"]=$basin_jobs\n",
    "      if [[ \"$slurm_script\" == \"moi.sh\" ]]; then\n",
    "        num_jobs=$basin_jobs\n",
    "      fi\n",
    "    fi\n",
    "\n",
    "    if [[ -s \"$metroman_json_file\" ]]; then\n",
    "      metroman_jobs=$(jq length \"$metroman_json_file\")\n",
    "      script_jobs[\"metroman.sh\"]=$metroman_jobs\n",
    "      if [[ \"$slurm_script\" == \"metroman.sh\"  ]]; then\n",
    "        num_jobs=$metroman_jobs\n",
    "      fi\n",
    "    fi\n",
    "\n",
    "    # Fallback: all remaining $default_jobs modules use reaches.json once available,\n",
    "    # otherwise fall back to reaches_of_interest.json\n",
    "    if [[ -z \"$num_jobs\" || \"$num_jobs\" == \"\\$default_jobs\" ]]; then\n",
    "        if [[ -s \"$reach_json_file\" ]]; then\n",
    "            num_jobs=$(jq length \"$reach_json_file\")\n",
    "            echo \"Using reach_json_file job count ($num_jobs) for $slurm_script\"\n",
    "        else\n",
    "            num_jobs=$default_jobs\n",
    "            echo \"Using reaches_of_interest.json job count ($num_jobs) for $slurm_script\"\n",
    "        fi\n",
    "    fi\n",
    "\n",
    "    # Safety check\n",
    "    if [[ -z \"$num_jobs\" ]]; then\n",
    "        echo \"Warning: No job count found for $slurm_script. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    start=0\n",
    "    while [ $start -lt $num_jobs ]; do\n",
    "        end=$((start + batch_size - 1))\n",
    "        if [ $end -ge $num_jobs ]; then\n",
    "            end=$((num_jobs - 1))\n",
    "        fi\n",
    "\n",
    "        echo \"Submitting jobs $start to $end from $slurm_script\"\n",
    "        job_id=$(sbatch --export=ALL,OFFSET=${{start}} --array=0-$((end - start))%${{concurrent_jobs}} \"${{directory}}/${{slurm_script}}\")\n",
    "        # job_id=$(sbatch --array=${{start}}-${{end}}%${{concurrent_jobs}} \"${{directory}}/${{slurm_script}}\")\n",
    "        job_id_number=$(echo $job_id | awk '{{print $4}}')\n",
    "\n",
    "        echo \"Waiting for job array $job_id_number to finish...\"\n",
    "        while squeue -j \"$job_id_number\" 2>/dev/null | grep -q \"$job_id_number\"; do\n",
    "            job_info=$(squeue -j \"${{job_id_number}}[]\" --noheader -o \"%i %T %R\")\n",
    "            held_tasks=$(echo \"$job_info\" | grep -i \"requeued held\" | awk '{{print $1}}')\n",
    "\n",
    "            if [[ -n \"$held_tasks\" ]]; then\n",
    "                echo \"Detected held tasks in array $job_id_number:\"\n",
    "                echo \"$held_tasks\"\n",
    "                for task in $held_tasks; do\n",
    "                    echo \"Cancelling task $task...\"\n",
    "                    scancel \"$task\"\n",
    "                done\n",
    "            fi\n",
    "\n",
    "            sleep 10\n",
    "        done\n",
    "\n",
    "        echo \"Batch $job_id_number has finished. Submitting next batch.\"\n",
    "        date\n",
    "\n",
    "        start=$((end + 1))\n",
    "        sleep 5\n",
    "    done      \n",
    "    \n",
    "done\n",
    "\n",
    "echo \"Run $run has finished successfully.\"\n",
    "\"\"\"\n",
    "    return slurm_header + body\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Run\n",
    "\n",
    "---\n",
    "* Assumes you have local Docker images built and pushed to DockerHub\n",
    "* Download or copy empty mnt and point to necessary directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess as sp\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Change these inputs to point wherever you need confluence to be on local machine\n",
    "\n",
    "BASE_DIR = Path('/path/confluence/') #directory storing confluence runs\n",
    "REPO_DIR = os.path.join(BASE_DIR, 'modules') #directory storing repos i.e ./modules/\n",
    "RUN_NAME = 'runTest' #Specific run name i.e. 'test'\n",
    "os.chdir(BASE_DIR)\n",
    "\n",
    "run_dir = BASE_DIR / f'confluence_{RUN_NAME}' # new directory for run\n",
    "src_dir = BASE_DIR / 'confluence_empty'\n",
    "\n",
    "#------------------------------------------------\n",
    "\n",
    "# SETUP, GitHub, DOCKER (DOCKER MUST BE OPEN)\n",
    "github_name = 'github_username' # GitHub username or organization name where repos are located\n",
    "push = True # Only select True if want to store images on dockerhub (one way to move to HPC)\n",
    "docker_username = 'docker_username'\n",
    "custom_tag_name = 'latest' # version control, will default to 'latest'\n",
    "run_list = [f'{RUN_NAME}'] #one name per mnt, can build if you preset multiple mnts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose modules of interest to run\n",
    "\n",
    "#Name of confluence offline module\n",
    "#expanded and non_expanded modules each work from single 'setfinder' and 'combine_data' module\n",
    "INCLUDED_MODULES = [\n",
    "    'expanded_setfinder',\n",
    "    'expanded_combine_data',\n",
    "    'input',\n",
    "    'non_expanded_setfinder',\n",
    "    'non_expanded_combine_data',\n",
    "    'prediagnostics',\n",
    "    # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'unconstrained_momma',\n",
    "    'hivdi',\n",
    "    # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # 'moi',\n",
    "    # 'unconstrained_offline',\n",
    "    # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n",
    "# Modules to pull/build\n",
    "TARGET_MODULES = [\n",
    "    'setfinder',\n",
    "    'combine_data',\n",
    "    'input',\n",
    "    'prediagnostics',\n",
    "    # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'momma',\n",
    "    'hivdi',\n",
    "    # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # 'moi',\n",
    "    # 'offline',\n",
    "    # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## INITIAL OR NEW MNT DOWNLOAD:\n",
    "# comment out after first install\n",
    "###############################\n",
    "\n",
    "## Install empty /mnt directory with input data and eventual output data\n",
    "# ! pip install gdown\n",
    "! gdown 10gJwg0wsl51K_mcoXGq1uQVW34oQwrJc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME: svs17\n",
      "REPO_DIR: /nas/cee-water/cjgleason/ellie/SWOT/confluence/modules/D\n",
      "SIF_DIR: /nas/cee-water/cjgleason/ellie/SWOT/confluence/confluence_svs17/sif\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## SUBSEQUENT RUNS:\n",
    "# Use this to make new empty directory to run new reaches\n",
    "####################\n",
    "\n",
    "## Extract from tar.gz\n",
    "tar_path = src_dir.with_suffix('.tar.gz')\n",
    "with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "    tar.extractall(path=src_dir.parent)\n",
    "\n",
    "# Rename to your run\n",
    "src_dir.rename(run_dir)  # Rename to your run directory\n",
    "p = run_dir / \"empty_mnt\" # rename internal mnt to run name\n",
    "p.rename(p.with_name(f\"{RUN_NAME}_mnt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to necessary directories \n",
    "SIF_DIR = run_dir / 'sif' # Store built Docker images\n",
    "sh_dir = run_dir / 'sh_scripts' # Store the sh scripts to run each module\n",
    "report_dir = run_dir / 'report' # Job logs\n",
    "mnt_dir = run_dir / f'{RUN_NAME}_mnt' #the mnt storing all confluence run data\n",
    "\n",
    "os.environ['RUN_NAME'] = RUN_NAME\n",
    "os.environ['BASE_DIR'] = str(BASE_DIR)\n",
    "os.environ['REPO_DIR'] = str(REPO_DIR)\n",
    "os.environ['SIF_DIR'] = str(SIF_DIR)\n",
    "\n",
    "print(f'RUN_NAME: {RUN_NAME}')\n",
    "print(f'REPO_DIR: {REPO_DIR}')\n",
    "print(f'SIF_DIR: {SIF_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create sh Scripts\n",
    "\n",
    "---\n",
    "### Confluence Module SLURM Script Generator (RUN ON HPC, NOT LOCALLY)\n",
    "* Build simg files from your dockerhub and generates scripts to submit to a SLURM job scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  hivdi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:    Starting build...\n",
      "INFO:    Fetching OCI image...\n",
      "INFO:    Extracting OCI image...\n",
      "INFO:    Inserting Apptainer configuration...\n",
      "INFO:    Creating SIF file...\n",
      "INFO:    Build complete: /nas/cee-water/cjgleason/ellie/SWOT/confluence/confluence_svs17/sif/hivdi.simg\n"
     ]
    }
   ],
   "source": [
    "# Build the scripts and singularity files\n",
    "\n",
    "build_sifs_and_create_slurm_scripts(run_list=run_list, \\\n",
    "                                    included_modules = TARGET_MODULES, \\\n",
    "                                    base_dir = BASE_DIR, \\\n",
    "                                    docker_username = docker_username, \\\n",
    "                                    build = push, \\\n",
    "                                    custom_tag_name = custom_tag_name\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Driver Script to run multiple modules\n",
    "\n",
    "---\n",
    "### Confluence Driver Script Generator (RUN ON HPC, NOT LOCALLY)\n",
    "* Creates a batch submission script that will run all of your sif files in serial\n",
    "* use sbatch to submit the entire run\n",
    "* low resources and a long time should be used here, as all this job will do is launch your SLURM scripts you created for each module, it is basically a job manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create driver SLURM script for each run in run_list\n",
    "\n",
    "# Define which modules have special (hardcoded) job counts\n",
    "HARDCODED_JOBS = {\n",
    "    \"expanded_setfinder\": \"6\",\n",
    "    \"expanded_combine_data\": \"1\",\n",
    "    \"non_expanded_setfinder\": \"6\",\n",
    "    \"non_expanded_combine_data\": \"1\",\n",
    "    \"unconstrained_priors\": \"6\",\n",
    "    \"constrained_priors\": \"6\",\n",
    "    \"metroman_consolidation\": \"6\",\n",
    "    \"output\": \"6\",\n",
    "}\n",
    "\n",
    "# Define modules that need dynamic job counts (will use $default_jobs placeholder)\n",
    "# These will be upgraded to specific JSON files during execution\n",
    "DYNAMIC_MODULES = [\n",
    "    \"input\",\n",
    "    \"prediagnostics\",\n",
    "    \"metroman\",\n",
    "    \"hivdi\",\n",
    "    \"sic4dvar\",\n",
    "    \"unconstrained_momma\",\n",
    "    \"constrained_momma\",\n",
    "    \"sad\",\n",
    "    \"moi\",\n",
    "    \"consensus\",\n",
    "    \"unconstrained_offline\",\n",
    "    \"validation\",\n",
    "]\n",
    "\n",
    "for run in run_list:\n",
    "\n",
    "    job_name = str(run)\n",
    "    output_log_dir = f\"{run_dir}/log\"\n",
    "    partition = \"cpu-preempt\" #your partition here\n",
    "    time_limit = \"30:00:00\"\n",
    "    nodes = 1\n",
    "    ntasks = 1\n",
    "    cpus_per_task = 1\n",
    "    mem = \"5G\"\n",
    "\n",
    "    run = str(run)\n",
    "    directory = run_dir\n",
    "    sh_directory = f\"{directory}/sh_scripts\"\n",
    "    json_file = f\"{directory}/{run}_mnt/input/reaches_of_interest.json\"\n",
    "    expanded_json_file = f\"{directory}/{run}_mnt/input/expanded_reaches_of_interest.json\"\n",
    "    reach_json_file = f\"{directory}/{run}_mnt/input/reaches.json\"\n",
    "    basin_json_file = f\"{directory}/{run}_mnt/input/basin.json\"\n",
    "    metroman_json_file = f\"{directory}/{run}_mnt/input/metrosets.json\"\n",
    "    \n",
    "\n",
    "    batch_size = 1000 # cluster specific\n",
    "    concurrent_jobs = 400 # cluster specific\n",
    "\n",
    "    # Dynamically build script_jobs based on INCLUDED_MODULES\n",
    "    script_jobs = {}\n",
    "    for module in INCLUDED_MODULES:\n",
    "        script_name = f\"{module}.sh\"\n",
    "        \n",
    "        if module in HARDCODED_JOBS:\n",
    "            # Use hardcoded job count\n",
    "            script_jobs[script_name] = HARDCODED_JOBS[module]\n",
    "        elif module in DYNAMIC_MODULES:\n",
    "            pass\n",
    "    \n",
    "    # Dynamically build scripts list (same order as INCLUDED_MODULES)\n",
    "    scripts = [f\"{module}.sh\" for module in INCLUDED_MODULES]\n",
    "    \n",
    "    driver_script = generate_slurm_driver(\n",
    "        job_name=job_name,\n",
    "        output_log_dir=output_log_dir,\n",
    "        partition=partition,\n",
    "        time_limit=time_limit,\n",
    "        nodes=nodes,\n",
    "        ntasks=ntasks,\n",
    "        cpus_per_task=cpus_per_task,\n",
    "        mem=mem,\n",
    "        run=run,\n",
    "        directory=sh_directory,\n",
    "        json_file=json_file,\n",
    "        expanded_json_file=expanded_json_file,\n",
    "        reach_json_file=reach_json_file,\n",
    "        basin_json_file=basin_json_file,\n",
    "        metroman_json_file=metroman_json_file,\n",
    "        batch_size=batch_size,\n",
    "        concurrent_jobs=concurrent_jobs,\n",
    "        script_jobs=script_jobs,\n",
    "        scripts=scripts,\n",
    "    )\n",
    "    \n",
    "    # Save to file\n",
    "    with open(f\"{sh_directory}/slurm_driver.sh\", \"w\") as f:\n",
    "        f.write(driver_script)\n",
    "\n",
    "\n",
    "# Optionally submit\n",
    "# import subprocess\n",
    "# subprocess.run([\"sbatch\", f\"{sh_dir}/slurm_driver.sh\"], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Reach or Module Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to run on different Type I reaches\n",
    "* modify the file at /mnt/input/reaches_of_interest.json\n",
    "\n",
    "#### In order to change a module and test it:\n",
    "### Option 1\n",
    "* change the module locally, build it and push to dockerhub using the first part of this notebook and then run as usual\n",
    "* you can use the run_list variable to generate more submission script per moule to test more than one change at a time. However, whenver you submit them, they will still run one at a time, it just submits the next run automatically.\n",
    "* Docker tag names highly recommended (custom_tag_name) for version control\n",
    "\n",
    "### Option 2\n",
    "* Use code below to generate everything in the HPC environment from cloning Git modules to running Confluence\n",
    "* Docker images are built initially using GitHub container registry (ghcr.io/) and then overwritten with your HPC modules \n",
    "* This allows you to change module, re-build containers, and test as a module instantly\n",
    "* Version control is handled by GitHub tag:\n",
    "* Tag your local image\n",
    "*      ! docker tag output:local ghcr.io/myGitAccount/moduleName:my-custom-tag\n",
    "* Push to registry\n",
    "*      docker push ghcr.io/myGitAccount/moduleName:my-custom-tag\n",
    "* Modify tag name in function from 'latest' to 'my-custom-tag'\n",
    "\n",
    "### Option 3\n",
    "* Use a symlink to connect a previous run to a new directory\n",
    "* Run module of interest using the data in previous modules (only need to run the changed module!)\n",
    "* Combine 2 and 3 for efficient testing of multiple changes to one or many modules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Option 2:\n",
    "## Run Confluence on an HPC end-to-end\n",
    "\n",
    "## Requirements\n",
    "* GitHub account\n",
    "* apptainer installed on your HPC\n",
    "* Basic python environment\n",
    "\n",
    "\n",
    "## Overall Tasks\n",
    "* Git clone all of the repos you want to run\n",
    "* Prep an empty_mnt directory to store confluence run (requires gdown package in environment)\n",
    "* Run image prep function to create the images from GitHub and your cloned modules\n",
    "* Create SLURM submission scripts for each module\n",
    "* Run the Confluence Driver Script Generator section of this notebook on your HPC to create a SLURM submission script that runs each of the modules one by one (the one click run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "## Functions (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repos(github_name, repo_dir, repo_names, name_map, branch='main'):\n",
    "    \"\"\"Clone repositories with specified branch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    github_name : str\n",
    "        GitHub username or organization name\n",
    "    repo_dir : str\n",
    "        Directory to clone repos into\n",
    "    repo_names : list\n",
    "        List of repository names to clone\n",
    "    branch : str or dict, optional\n",
    "        Branch name to clone. Can be:\n",
    "        - A string: same branch for all repos (default: 'main')\n",
    "        - A dict: mapping repo name to specific branch\n",
    "    \"\"\"\n",
    "    os.makedirs(repo_dir, exist_ok=True)\n",
    "    \n",
    "    for name in repo_names:\n",
    "        path = os.path.join(repo_dir, name)\n",
    "        repo_name = name_map.get(name, name)\n",
    "        url = f'https://github.com/{github_name}/{repo_name}.git'\n",
    "        \n",
    "        # Determine which branch to use\n",
    "        if isinstance(branch, dict):\n",
    "            branch_name = branch.get(name, 'main')\n",
    "        else:\n",
    "            branch_name = branch\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            print(f'[Remove] Deleting existing {name} to overwrite...')\n",
    "            try:\n",
    "                shutil.rmtree(path)  # rm -rf\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {path} : {e.strerror}\")\n",
    "        \n",
    "        print(f'[Clone] Cloning {name} from branch {branch_name}...')\n",
    "        sp.run(['git', 'clone', '--branch', branch_name, url, name], cwd=repo_dir)\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "def create_slurm_scripts(run_name, mnt_dir, sif_dir, sh_dir, report_dir, included_modules):\n",
    "    # Create batchs script details\n",
    "    submission_prefix = '#SBATCH'\n",
    "\n",
    "    job_details = {\n",
    "    'partition': 'cpu-preempt',\n",
    "    'nodes' : '1',\n",
    "    'cpus-per-task': '1',\n",
    "    'job-name': f'{run_name}_cfl',\n",
    "    }\n",
    "\n",
    "    command_dict = {\n",
    "        'expanded_setfinder': f'apptainer run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.sif') + ' -r reaches_of_interest.json -c continent.json -e -s 17 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "        'expanded_combine_data': f'apptainer run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.sif') + ' -d /data  -e -s 17',\n",
    "        'input': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind ' + f'{mnt_dir}/input:/mnt/data ' + os.path.join(sif_dir, 'input.sif') + ' -v 17 -r /mnt/data/expanded_reaches_of_interest.json -c SWOT_L2_HR_RiverSP_D -i ${GLOBAL_INDEX}',\n",
    "        'non_expanded_setfinder': f'apptainer run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'setfinder.sif') + ' -c continent.json -s 17 -o /data -n /data -a MetroMan HiVDI SIC NeoBAM -i ${SLURM_ARRAY_TASK_ID}',\n",
    "        'non_expanded_combine_data': f'apptainer run --bind ' + f'{mnt_dir}/input:/data ' + os.path.join(sif_dir, 'combine_data.sif') + ' -d /data -s 17',\n",
    "        'prediagnostics': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind ' + f'{mnt_dir}/input:/mnt/data/input,{mnt_dir}/diagnostics/prediagnostics:/mnt/data/output ' + os.path.join(sif_dir, f'prediagnostics.sif') + ' -i ${GLOBAL_INDEX} -r reaches.json',\n",
    "        'constrained_priors': f'apptainer run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.sif\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r constrained -p usgs riggs -g -s local',\n",
    "        'unconstrained_priors': f'apptainer run -c --writable-tmpfs --bind {mnt_dir}/input:/mnt/data {os.path.join(sif_dir, \"priors.sif\")} ' + ' -i ${SLURM_ARRAY_TASK_ID} -r unconstrained -p usgs riggs -g -s local',\n",
    "        'hivdi': f'singularity run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/hivdi:/mnt/data/flpe/hivdi ' + os.path.join(sif_dir, 'hivdi.simg') + ' /mnt/data/input/reaches.json --input-dir /mnt/data/input -i ${SLURM_ARRAY_TASK_ID}',\n",
    "        'sic4dvar': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sic4dvar:/mnt/data/output,{mnt_dir}/logs:/mnt/data/logs '+ os.path.join(sif_dir, 'sic4dvar.sif') + ' -r reaches.json --index ${GLOBAL_INDEX}',\n",
    "        'metroman': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/output ' + os.path.join(sif_dir, \"metroman.sif\") + ' -i ${GLOBAL_INDEX} -r metrosets.json -s local -v',\n",
    "        'metroman_consolidation': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/metroman:/mnt/data/flpe ' + os.path.join(sif_dir, 'metroman_consolidation.sif') + ' -i ${GLOBAL_INDEX}',\n",
    "        'unconstrained_momma': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/momma:/mnt/data/output ' + os.path.join(sif_dir, 'momma.sif') + ' -r reaches.json -m 3 -i ${GLOBAL_INDEX}',\n",
    "        'constrained_momma': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/momma:/mnt/data/output ' + os.path.join(sif_dir, 'momma.sif') + ' -r reaches.json -m 3 -c -i ${GLOBAL_INDEX}',\n",
    "        'sad': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe/sad:/mnt/data/output ' + os.path.join(sif_dir, 'sad.sif') + ' --reachfile reaches.json --index ${GLOBAL_INDEX}',\n",
    "        'moi': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --env AWS_BATCH_JOB_ID=\"foo\" --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/output ' + os.path.join(sif_dir, 'moi.sif') + ' -j basin.json -v -b unconstrained -i ${GLOBAL_INDEX}', # -s local\n",
    "        'consensus': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe ' + os.path.join(sif_dir, 'consensus.sif') + ' --mntdir /mnt/data -r /mnt/data/input/reaches.json -i ${GLOBAL_INDEX}',\n",
    "        'unconstrained_offline': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/output ' + os.path.join(sif_dir, 'offline.sif') + ' unconstrained timeseries integrator reaches.json ${GLOBAL_INDEX}',\n",
    "        'validation': f'GLOBAL_INDEX=$(( ${{OFFSET:-0}} + SLURM_ARRAY_TASK_ID ))\\n\\napptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/output ' + os.path.join(sif_dir, 'validation.sif') + ' -r reaches.json -t unconstrained -i ${GLOBAL_INDEX}',\n",
    "        # 'output': f'apptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/diagnostics:/mnt/data/diagnostics,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/validation,{mnt_dir}/output:/mnt/data/output ' + os.path.join(sif_dir, 'output.sif') + ' -s local -j /app/metadata/metadata.json -m input prediagnostics momma hivdi neobam metroman sic4dvar sad consensus validation swot priors -v 17 -i ${SLURM_ARRAY_TASK_ID}'\n",
    "        'output': f'apptainer run --bind {mnt_dir}/input:/mnt/data/input,{mnt_dir}/flpe:/mnt/data/flpe,{mnt_dir}/diagnostics:/mnt/data/diagnostics,{mnt_dir}/moi:/mnt/data/moi,{mnt_dir}/offline:/mnt/data/offline,{mnt_dir}/validation:/mnt/data/validation,{mnt_dir}/output:/mnt/data/output ' + os.path.join(sif_dir, 'output.sif') + ' -s local -j /app/metadata/metadata.json -m input momma metroman sic4dvar hivdi consensus swot -v 17 -i ${SLURM_ARRAY_TASK_ID}'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    def create_slurm_script(job_details=job_details):\n",
    "        submission_prefix = job_details['submission_prefix']\n",
    "\n",
    "        file = open(os.path.join(sh_dir, f'{module_to_run}.sh'), 'w')\n",
    "        file.write('#!/bin/bash \\n')\n",
    "        file.write(f'{submission_prefix} -o {os.path.join(report_dir, f\"{module_to_run}.%j_%a.out\")}' + ' \\n')\n",
    "\n",
    "        for item in job_details:\n",
    "            if item not in ['run_command', 'module_name', 'submission_prefix']:\n",
    "                file.write(f'{submission_prefix} --{item}={job_details[item]} \\n')\n",
    "        file.write(job_details[\"run_command\"])\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    for module_to_run, run_command in command_dict.items():\n",
    "\n",
    "        if module_to_run == 'moi':\n",
    "            time_to_use = '00:30:00'\n",
    "            mem_to_use = '2G'\n",
    "        elif module_to_run == 'output':\n",
    "            time_to_use = '05:00:00'\n",
    "            mem_to_use = '4G'\n",
    "        else:\n",
    "            time_to_use = '00:20:00'\n",
    "            mem_to_use = '4G'\n",
    "\n",
    "        if included_modules:\n",
    "            if module_to_run not in included_modules:\n",
    "                continue\n",
    "\n",
    "        print('DIRECTORY NAME: ', run_name, '\\nMODULE: ', module_to_run)\n",
    "\n",
    "\n",
    "\n",
    "        job_details.update({\n",
    "            'run_command': run_command,\n",
    "            'module_name': module_to_run,\n",
    "            'mem': mem_to_use,\n",
    "            'time': time_to_use,\n",
    "            'submission_prefix': submission_prefix,\n",
    "            'job-name': f'{module_to_run}_{run_name}_cfl',\n",
    "\n",
    "        })\n",
    "\n",
    "        create_slurm_script(job_details=job_details)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "def create_singularity_def(mod, repo_dir):\n",
    "    \"\"\"Dockerfile -> Singularity.def with fix for nested output\"\"\"\n",
    "    \n",
    "    dockerfile_path = os.path.join(repo_dir, mod, 'Dockerfile')\n",
    "    def_path = os.path.join(repo_dir, mod, 'Singularity.def')\n",
    "    \n",
    "    if not os.path.exists(dockerfile_path):\n",
    "        print(f'{mod}: Dockerfile x, skip')\n",
    "        return None\n",
    "    \n",
    "    with open(dockerfile_path) as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    files = []\n",
    "    entrypoint = None\n",
    "    \n",
    "    for line in content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line.startswith('COPY'):\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3 and '--from' not in line:\n",
    "                src = parts[1]\n",
    "                dst = parts[2]\n",
    "                if 'requirements' not in src:\n",
    "                    files.append((src.replace('./', ''), dst))\n",
    "        \n",
    "        if line.startswith('ENTRYPOINT'):\n",
    "            if '[' in line:\n",
    "                import re\n",
    "                matches = re.findall(r'\"([^\"]*)\"', line)\n",
    "                if matches:\n",
    "                    entrypoint = ' '.join(matches)\n",
    "    \n",
    "    # Singularity.def with post-processing fix\n",
    "    def_content = f'''Bootstrap: docker\n",
    "From: ghcr.io/swot-confluence/{mod}:latest\n",
    "\n",
    "%files\n",
    "'''\n",
    "    for src, dst in files:\n",
    "        def_content += f'    {src} {dst}\\n'\n",
    "    \n",
    "    # Add post section to fix nested directories\n",
    "    if mod == 'output':\n",
    "        def_content += '''\n",
    "%post\n",
    "    # Fix nested output directory - copy contents up one level\n",
    "    if [ -d /app/output/output ]; then\n",
    "        cp -rf /app/output/output/* /app/output/\n",
    "        rm -rf /app/output/output\n",
    "    fi\n",
    "'''\n",
    "    \n",
    "    if entrypoint:\n",
    "        def_content += f'''\n",
    "%runscript\n",
    "    exec {entrypoint} \"$@\"\n",
    "'''\n",
    "    \n",
    "    with open(def_path, 'w') as f:\n",
    "        f.write(def_content)\n",
    "    \n",
    "    print(f'{mod}: Singularity.def created')\n",
    "    return def_path\n",
    "\n",
    "#------------------------------------------------------------\n",
    "                \n",
    "def generate_slurm_driver(\n",
    "    job_name: str,\n",
    "    output_log_dir: str,\n",
    "    partition: str,\n",
    "    time_limit: str,\n",
    "    nodes: int,\n",
    "    ntasks: int,\n",
    "    cpus_per_task: int,\n",
    "    mem: str,\n",
    "    run: str,\n",
    "    directory: str,\n",
    "    json_file: str,\n",
    "    expanded_json_file: str,\n",
    "    reach_json_file: str,\n",
    "    basin_json_file: str,\n",
    "    metroman_json_file: str,\n",
    "    batch_size: int,\n",
    "    concurrent_jobs: int,\n",
    "    script_jobs: dict[str, str],\n",
    "    scripts: list[str]\n",
    ") -> str:\n",
    "    slurm_header = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --output={output_log_dir}/{job_name}_%j_%a.out\n",
    "#SBATCH --error={output_log_dir}/{job_name}_%j_%a.err\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --time={time_limit}\n",
    "#SBATCH --nodes={nodes}\n",
    "#SBATCH --ntasks={ntasks}\n",
    "#SBATCH --cpus-per-task={cpus_per_task}\n",
    "#SBATCH --mem={mem}\n",
    "\n",
    "run='{run}'\n",
    "echo \"Run: $run\"\n",
    "\n",
    "directory=\"{directory}\"\n",
    "\n",
    "# Parameters\n",
    "json_file=\"{json_file}\"\n",
    "expanded_json_file=\"{expanded_json_file}\"\n",
    "reach_json_file=\"{reach_json_file}\"\n",
    "basin_json_file=\"{basin_json_file}\"\n",
    "metroman_json_file=\"{metroman_json_file}\"\n",
    "default_jobs=$(jq length \"$json_file\")\n",
    "\n",
    "# Adjust to HPC requirements\n",
    "batch_size={batch_size}\n",
    "concurrent_jobs={concurrent_jobs}\n",
    "\n",
    "# Map specific script names to their job counts\n",
    "declare -A script_jobs=(\n",
    "\"\"\"\n",
    "\n",
    "    # Inject job counts into script_jobs associative array\n",
    "    for script, jobs in script_jobs.items():\n",
    "        slurm_header += f\"    [{script}]={jobs}\\n\"\n",
    "    slurm_header += \")\\n\\n\"\n",
    "\n",
    "    # Build scripts array\n",
    "    script_array = '    ' + '\\n    '.join(scripts)\n",
    "    scripts_block = f\"\"\"scripts=(\n",
    "{script_array}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "    body = rf\"\"\"{scripts_block}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for slurm_script in \"${{scripts[@]}}\"; do\n",
    "    echo \"Starting submission for: $slurm_script\"\n",
    "    date\n",
    "\n",
    "    # Initialize num_jobs from script_jobs array FIRST\n",
    "    num_jobs=\"${{script_jobs[$slurm_script]}}\"\n",
    "\n",
    "    # Dynamic job count updates (files created during workflow)\n",
    "    if [[ -s \"$expanded_json_file\" ]]; then\n",
    "      expanded_jobs=$(jq length \"$expanded_json_file\")\n",
    "      script_jobs[\"input.sh\"]=$expanded_jobs\n",
    "      # Update num_jobs if this is the input script\n",
    "      if [[ \"$slurm_script\" == \"input.sh\" ]]; then\n",
    "        num_jobs=$expanded_jobs\n",
    "      fi\n",
    "    fi\n",
    "\n",
    "    if [[ -s \"$basin_json_file\" ]]; then\n",
    "      basin_jobs=$(jq length \"$basin_json_file\")\n",
    "      script_jobs[\"moi.sh\"]=$basin_jobs\n",
    "      if [[ \"$slurm_script\" == \"moi.sh\" ]]; then\n",
    "        num_jobs=$basin_jobs\n",
    "      fi\n",
    "    fi\n",
    "\n",
    "    if [[ -s \"$metroman_json_file\" ]]; then\n",
    "      metroman_jobs=$(jq length \"$metroman_json_file\")\n",
    "      script_jobs[\"metroman.sh\"]=$metroman_jobs\n",
    "      if [[ \"$slurm_script\" == \"metroman.sh\"  ]]; then\n",
    "        num_jobs=$metroman_jobs\n",
    "      fi\n",
    "    fi\n",
    "\n",
    "    # Fallback: all remaining $default_jobs modules use reaches.json once available,\n",
    "    # otherwise fall back to reaches_of_interest.json\n",
    "    if [[ -z \"$num_jobs\" || \"$num_jobs\" == \"\\$default_jobs\" ]]; then\n",
    "        if [[ -s \"$reach_json_file\" ]]; then\n",
    "            num_jobs=$(jq length \"$reach_json_file\")\n",
    "            echo \"Using reach_json_file job count ($num_jobs) for $slurm_script\"\n",
    "        else\n",
    "            num_jobs=$default_jobs\n",
    "            echo \"Using reaches_of_interest.json job count ($num_jobs) for $slurm_script\"\n",
    "        fi\n",
    "    fi\n",
    "\n",
    "    # Safety check\n",
    "    if [[ -z \"$num_jobs\" ]]; then\n",
    "        echo \"Warning: No job count found for $slurm_script. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    start=0\n",
    "    while [ $start -lt $num_jobs ]; do\n",
    "        end=$((start + batch_size - 1))\n",
    "        if [ $end -ge $num_jobs ]; then\n",
    "            end=$((num_jobs - 1))\n",
    "        fi\n",
    "\n",
    "        echo \"Submitting jobs $start to $end from $slurm_script\"\n",
    "        job_id=$(sbatch --export=ALL,OFFSET=${{start}} --array=0-$((end - start))%${{concurrent_jobs}} \"${{directory}}/${{slurm_script}}\")\n",
    "        # job_id=$(sbatch --array=${{start}}-${{end}}%${{concurrent_jobs}} \"${{directory}}/${{slurm_script}}\")\n",
    "        job_id_number=$(echo $job_id | awk '{{print $4}}')\n",
    "\n",
    "        echo \"Waiting for job array $job_id_number to finish...\"\n",
    "        while squeue -j \"$job_id_number\" 2>/dev/null | grep -q \"$job_id_number\"; do\n",
    "            job_info=$(squeue -j \"${{job_id_number}}[]\" --noheader -o \"%i %T %R\")\n",
    "            held_tasks=$(echo \"$job_info\" | grep -i \"requeued held\" | awk '{{print $1}}')\n",
    "\n",
    "            if [[ -n \"$held_tasks\" ]]; then\n",
    "                echo \"Detected held tasks in array $job_id_number:\"\n",
    "                echo \"$held_tasks\"\n",
    "                for task in $held_tasks; do\n",
    "                    echo \"Cancelling task $task...\"\n",
    "                    scancel \"$task\"\n",
    "                done\n",
    "            fi\n",
    "\n",
    "            sleep 10\n",
    "        done\n",
    "\n",
    "        echo \"Batch $job_id_number has finished. Submitting next batch.\"\n",
    "        date\n",
    "\n",
    "        start=$((end + 1))\n",
    "        sleep 5\n",
    "    done      \n",
    "    \n",
    "done\n",
    "\n",
    "echo \"Run $run has finished successfully.\"\n",
    "\"\"\"\n",
    "    return slurm_header + body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess as sp\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path('/path/confluence/') #directory storing confluence runs\n",
    "REPO_DIR = BASE_DIR / 'modules/' #directory storing repos i.e ./modules/\n",
    "RUN_NAME = 'runTest' #Specific run name i.e. 'runtest'\n",
    "\n",
    "run_dir = BASE_DIR / f'confluence_{RUN_NAME}' # new directory for run\n",
    "src_dir = BASE_DIR / 'confluence_empty'\n",
    "\n",
    "HPC_username = 'your_hpc_username'\n",
    "github_name = 'your_github_name'\n",
    "os.chdir(BASE_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## INITIAL OR NEW MNT DOWNLOAD:\n",
    "###############################\n",
    "\n",
    "## Install empty /mnt directory with input data and eventual output data\n",
    "# ! pip install gdown\n",
    "! gdown 10gJwg0wsl51K_mcoXGq1uQVW34oQwrJc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME: svs17\n",
      "REPO_DIR: /nas/cee-water/cjgleason/ellie/SWOT/confluence/modules/D\n",
      "SIF_DIR: /nas/cee-water/cjgleason/ellie/SWOT/confluence/confluence_svs17/sif\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## SUBSEQUENT RUNS:\n",
    "####################\n",
    "\n",
    "src_dir = os.path.join(BASE_DIR, 'confluence_empty')  # initial unzipped gdown \n",
    "run_dir = os.path.join(BASE_DIR, f'confluence_{RUN_NAME}') # new directory for run\n",
    "shutil.copytree(src_dir, run_dir) # copy the contents of empty to new (preserves initial data)\n",
    "\n",
    "p = Path(f\"{run_dir}/empty_mnt\") # rename internal mnt to run name\n",
    "p.rename(p.with_name(f\"{RUN_NAME}_mnt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Point to necessary directories \n",
    "SIF_DIR = run_dir / 'sif' # Store built Docker images\n",
    "sh_dir = run_dir / 'sh_scripts' # Store the sh scripts to run each module\n",
    "report_dir = run_dir / 'report' # Job logs\n",
    "mnt_dir = run_dir / f'{RUN_NAME}_mnt' #the mnt storing all confluence run data\n",
    "\n",
    "os.environ['RUN_NAME'] = RUN_NAME\n",
    "os.environ['BASE_DIR'] = str(BASE_DIR)\n",
    "os.environ['REPO_DIR'] = str(REPO_DIR)\n",
    "os.environ['SIF_DIR'] = str(SIF_DIR)\n",
    "os.environ['APPTAINER_CACHEDIR'] = f'/work/{HPC_username}/.apptainer/cache' #add your hpc username here\n",
    "\n",
    "# Fail safe for directory build\n",
    "for d in [SIF_DIR, sh_dir, report_dir, REPO_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f'RUN_NAME: {RUN_NAME}')\n",
    "print(f'REPO_DIR: {REPO_DIR}')\n",
    "print(f'SIF_DIR: {SIF_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Name of confluence offline module\n",
    "#expanded and non_expanded modules work from 'setfinder' and 'combine_data'\n",
    "INCLUDED_MODULES = [\n",
    "    'expanded_setfinder',\n",
    "    'expanded_combine_data',\n",
    "    'input',\n",
    "    'non_expanded_setfinder',\n",
    "    'non_expanded_combine_data',\n",
    "    'prediagnostics',\n",
    "    # # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'unconstrained_momma',\n",
    "    'hivdi',\n",
    "    # # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # # 'moi',\n",
    "    # # 'unconstrained_offline',\n",
    "    # # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n",
    "# Git modules to pull\n",
    "TARGET_MODULES = [\n",
    "    'setfinder',\n",
    "    'combine_data',\n",
    "    'input',\n",
    "    'prediagnostics',\n",
    "    # # 'priors',\n",
    "    'metroman',\n",
    "    'metroman_consolidation',\n",
    "    'momma',\n",
    "    'hivdi',\n",
    "    # # 'sad',\n",
    "    'sic4dvar',\n",
    "    'consensus',\n",
    "    # # 'moi',\n",
    "    # # 'offline',\n",
    "    # # 'validation',\n",
    "    'output'\n",
    "]\n",
    "\n",
    "# Pull working branches for certain Git repos\n",
    "branch_map = {\n",
    "    'setfinder': 'main',\n",
    "    'combine_data': 'main',\n",
    "    'input': 'input_D_products',\n",
    "    'prediagnostics': 'main',\n",
    "    # 'priors': 'main',\n",
    "    'metroman': 'main',\n",
    "    'metroman_consolidation': 'main',\n",
    "    'momma': 'main',\n",
    "    'h2ivdi': 'main',\n",
    "    # 'sad': 'main',\n",
    "    'sic4dvar': 'main',\n",
    "    'consensus': 'main',\n",
    "    # 'moi': 'main',\n",
    "    # 'offline': 'main',\n",
    "    # 'validation': 'main',\n",
    "    'output': 'add-sword-version'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone GitHub repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Remove] Deleting existing metroman to overwrite...\n",
      "[Clone] Cloning metroman from branch 16-discharge-timeseries-are-super-noisy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'metroman'...\n"
     ]
    }
   ],
   "source": [
    "name_map = {\n",
    "        'offline': 'offline-discharge-data-product-creation',\n",
    "        'moi': 'MOI',\n",
    "        'validation': 'Validation',\n",
    "        'hivdi': 'h2ivdi'\n",
    "    }\n",
    "clone_repos(github_name=github_name, repo_dir=REPO_DIR, repo_names=TARGET_MODULES, name_map=name_map, branch=branch_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build SIF (apptainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metroman]\n",
      "FROM python:3.12-slim as stage0\n",
      "FROM stage0 as stage1\n",
      "FROM stage1 as stage2\n",
      "COPY requirements.txt /app/requirements.txt\n",
      "FROM stage2 as stage3\n",
      "COPY ./metroman /app/metroman/\n",
      "COPY ./sos_read /app/sos_read/\n",
      "FROM stage3 as stage4\n",
      "COPY run_metroman.py /app/run_metroman.py\n",
      "ENTRYPOINT [\"/app/env/bin/python3\", \"/app/run_metroman.py\"]\n",
      "\n",
      "metroman: Singularity.def created\n",
      "metroman: Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:    User not listed in /etc/subuid, trying root-mapped namespace\n",
      "INFO:    fakeroot command not found\n",
      "INFO:    Installing some packages may fail\n",
      "INFO:    Starting build...\n",
      "INFO:    Fetching OCI image...\n",
      "INFO:    Extracting OCI image...\n",
      "INFO:    Inserting Apptainer configuration...\n",
      "INFO:    Copying metroman to /app/metroman/\n",
      "INFO:    Copying sos_read to /app/sos_read/\n",
      "INFO:    Copying run_metroman.py to /app/run_metroman.py\n",
      "INFO:    Adding runscript\n",
      "INFO:    Creating SIF file...\n",
      "INFO:    Build complete: /nas/cee-water/cjgleason/ellie/SWOT/confluence/confluence_svs17/sif/metroman.sif\n"
     ]
    }
   ],
   "source": [
    "# Builds container and sif for all repos of interest\n",
    "\n",
    "for mod in TARGET_MODULES:\n",
    "    dockerfile = os.path.join(REPO_DIR, mod, 'Dockerfile')\n",
    "    if os.path.exists(dockerfile):\n",
    "        print(f\"[{mod}]\")\n",
    "        with open(dockerfile) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                if line.strip().startswith(('COPY', 'ENTRYPOINT', 'FROM')):\n",
    "                    print(line.strip())\n",
    "        print()\n",
    "    else:\n",
    "        print(f'{mod} -> (Dockerfile x)')\n",
    "        print()\n",
    "        \n",
    "\n",
    "for mod in TARGET_MODULES:\n",
    "    create_singularity_def(mod, REPO_DIR)\n",
    "\n",
    "for mod in TARGET_MODULES:\n",
    "    sif_path = os.path.join(SIF_DIR, f'{mod}.sif')\n",
    "    print(f'{mod}: Building...')\n",
    "    os.system(f'cd {REPO_DIR}/{mod} && apptainer build --force --ignore-fakeroot-command {sif_path} Singularity.def')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create sh Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  prediagnostics\n",
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  sic4dvar\n",
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  metroman\n",
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  metroman_consolidation\n",
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  unconstrained_momma\n",
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  consensus\n",
      "DIRECTORY NAME:  svs17 \n",
      "MODULE:  output\n"
     ]
    }
   ],
   "source": [
    "create_slurm_scripts(run_name=RUN_NAME, \\\n",
    "                     mnt_dir=mnt_dir, \\\n",
    "                     sif_dir=SIF_DIR, \\\n",
    "                     sh_dir=sh_dir, \\\n",
    "                     report_dir=report_dir, \\\n",
    "                     included_modules=INCLUDED_MODULES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Driver Script to run multiple modules\n",
    "---\n",
    "### Confluence Driver Script Generator (RUN ON HPC, NOT LOCALLY)\n",
    "* Creates a batch submission script that will run all of your sif files in serial\n",
    "* use sbatch to submit the entire run\n",
    "* low resources and a long time should be used here, as all this job will do is launch your SLURM scripts you created for each module, it is basically a job manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create driver SLURM script for each run in run_list\n",
    "\n",
    "# Define which modules have special (hardcoded) job counts\n",
    "HARDCODED_JOBS = {\n",
    "    \"expanded_setfinder\": \"6\",\n",
    "    \"expanded_combine_data\": \"1\",\n",
    "    \"non_expanded_setfinder\": \"6\",\n",
    "    \"non_expanded_combine_data\": \"1\",\n",
    "    \"unconstrained_priors\": \"6\",\n",
    "    \"constrained_priors\": \"6\",\n",
    "    \"metroman_consolidation\": \"6\",\n",
    "    \"output\": \"6\",\n",
    "}\n",
    "\n",
    "# Define modules that need dynamic job counts (will use $default_jobs placeholder)\n",
    "# These will be upgraded to specific JSON files during execution\n",
    "DYNAMIC_MODULES = [\n",
    "    \"input\",\n",
    "    \"prediagnostics\",\n",
    "    \"metroman\",\n",
    "    \"hivdi\",\n",
    "    \"sic4dvar\",\n",
    "    \"unconstrained_momma\",\n",
    "    \"constrained_momma\",\n",
    "    \"sad\",\n",
    "    \"moi\",\n",
    "    \"consensus\",\n",
    "    \"unconstrained_offline\",\n",
    "    \"validation\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "job_name = str(RUN_NAME)\n",
    "output_log_dir = f\"{run_dir}/log\"\n",
    "partition = \"cpu-preempt\" #your partition here\n",
    "time_limit = \"30:00:00\"\n",
    "nodes = 1\n",
    "ntasks = 1\n",
    "cpus_per_task = 1\n",
    "mem = \"10G\"\n",
    "\n",
    "directory = run_dir\n",
    "sh_directory = f\"{directory}/sh_scripts\"\n",
    "json_file = f\"{directory}/{RUN_NAME}_mnt/input/reaches_of_interest.json\"\n",
    "expanded_json_file = f\"{directory}/{RUN_NAME}_mnt/input/expanded_reaches_of_interest.json\"\n",
    "reach_json_file = f\"{directory}/{RUN_NAME}_mnt/input/reaches.json\"\n",
    "basin_json_file = f\"{directory}/{RUN_NAME}_mnt/input/basin.json\"\n",
    "metroman_json_file = f\"{directory}/{RUN_NAME}_mnt/input/metrosets.json\"\n",
    "\n",
    "\n",
    "batch_size = 1000 # cluster specific\n",
    "concurrent_jobs = 400 # cluster specific\n",
    "\n",
    "# Dynamically build script_jobs based on INCLUDED_MODULES\n",
    "script_jobs = {}\n",
    "for module in INCLUDED_MODULES:\n",
    "    script_name = f\"{module}.sh\"\n",
    "\n",
    "    if module in HARDCODED_JOBS:\n",
    "        # Use hardcoded job count\n",
    "        script_jobs[script_name] = HARDCODED_JOBS[module]\n",
    "    elif module in DYNAMIC_MODULES:\n",
    "        pass\n",
    "        # Use $default_jobs placeholder (will be upgraded during execution)\n",
    "        #script_jobs[script_name] = \"$default_jobs\"\n",
    "    # If module not in either list, it won't be in script_jobs and will use fallback\n",
    "\n",
    "# Dynamically build scripts list (same order as INCLUDED_MODULES)\n",
    "scripts = [f\"{module}.sh\" for module in INCLUDED_MODULES]\n",
    "\n",
    "driver_script = generate_slurm_driver(\n",
    "    job_name=job_name,\n",
    "    output_log_dir=output_log_dir,\n",
    "    partition=partition,\n",
    "    time_limit=time_limit,\n",
    "    nodes=nodes,\n",
    "    ntasks=ntasks,\n",
    "    cpus_per_task=cpus_per_task,\n",
    "    mem=mem,\n",
    "    run=RUN_NAME,\n",
    "    directory=sh_directory,\n",
    "    json_file=json_file,\n",
    "    expanded_json_file=expanded_json_file,\n",
    "    reach_json_file=reach_json_file,\n",
    "    basin_json_file=basin_json_file,\n",
    "    metroman_json_file=metroman_json_file,\n",
    "    batch_size=batch_size,\n",
    "    concurrent_jobs=concurrent_jobs,\n",
    "    script_jobs=script_jobs,\n",
    "    scripts=scripts,\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "with open(f\"{sh_directory}/slurm_driver.sh\", \"w\") as f:\n",
    "    f.write(driver_script)\n",
    "\n",
    "\n",
    "# Optionally submit within notebook\n",
    "# import subprocess\n",
    "# subprocess.run([\"sbatch\", f\"{sh_dir}/slurm_driver.sh\"], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-swotEF]",
   "language": "python",
   "name": "conda-env-.conda-swotEF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
